{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Open Topo Data Open Topo Data is an elevation API. Host your own or use the free public API . Open Topo Data is a REST API server for your elevation data. curl https://api.opentopodata.org/v1/test-dataset?locations=56,123 { \"results\" : [{ \"elevation\" : 815.0 , \"location\" : { \"lat\" : 56.0 , \"lng\" : 123.0 }, \"dataset\" : \"test-dataset\" }], \"status\" : \"OK\" } You can self-host with your own dataset or use the free public API which is configured with a number of open elevation datasets. The API is largely compatible with the Google Maps Elevation API. Host your own Install docker and git then run: git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata make build make run This will start an Open Topo Data server on http://localhost:5000/ . Open Topo Data supports a wide range of raster file formats and tiling schemes, including most of those used by popular open elevation datasets. See the server docs for more about configuration, adding datasets, and running on Windows and M1 Macs. Usage Open Topo Data has a single endpoint: a point query endpoint that returns the elevation at a single point or a series of points. curl https://api.opentopodata.org/v1/test-dataset?locations=56.35,123.90 { \"results\" : [{ \"elevation\" : 815.0 , \"location\" : { \"lat\" : 56.0 , \"lng\" : 123.0 }, \"dataset\" : \"test-dataset\" }], \"status\" : \"OK\" } The interpolation algorithm used can be configured as a request parameter, and locations can also be provided in Google Polyline format. See the API docs for more about request and response formats. Public API I'm hosting a free public API at api.opentopodata.org . To keep the public API sustainable some limitations are applied. Max 100 locations per request. Max 1 call per second. Max 1000 calls per day. The following datasets are available on the public API, with elevation shown for downtown Denver, Colorado (39.7471, -104.9963). Dataset name Resolution Extent Source API link (Denver, CO) nzdem8m 8 m New Zealand. LINZ Not in dataset bounds ned10m ~10 m Continental USA, Hawaii, parts of Alaska. USGS 1590 m eudem25m 25 m Europe. EEA Not in dataset bounds mapzen ~30 m Global, inluding bathymetry. Mapzen 1590 m aster30m ~30 m Global. NASA 1591 m srtm30m ~30 m Latitudes -60 to 60. USGS 1604 m srtm90m ~90 m Latitudes -60 to 60. USGS 1603 m bkg200m 200 m Germany. BKG Not in dataset bounds etopo1 ~1.8 km Global, including bathymetry and ice surface elevation near poles. NOAA 1596 m gebco2020 ~450m Global bathymetry and land elevation. GEBCO 1603 m emod2018 ~100m Bathymetry for ocean and sea in Europe. EMODnet Not in dataset bounds See the API docs for more about request formats and parameters. Support Need some help getting Open Topo Data running? Send me an email at andrew@opentopodata.org ! Paid hosting If you need an elevation API service with high-quality 1m lidar data, check out my sister project GPXZ . The GPXZ Elevation API offers the following features: Managed hosting, load balanced for redundancy Seamless, global, hi-res elevation dataset Drop-in replacement endpoint for the Google Maps Elevation API Priority support No hard usage limits EU-only servers if needed CORS (so you can use the API in your frontend webapp) For more details, reach out to andrew@opentopodata.org . Paid hosting funds the development of Open Topo Data and keeps the public API free.","title":"Introduction"},{"location":"#host-your-own","text":"Install docker and git then run: git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata make build make run This will start an Open Topo Data server on http://localhost:5000/ . Open Topo Data supports a wide range of raster file formats and tiling schemes, including most of those used by popular open elevation datasets. See the server docs for more about configuration, adding datasets, and running on Windows and M1 Macs.","title":"Host your own"},{"location":"#usage","text":"Open Topo Data has a single endpoint: a point query endpoint that returns the elevation at a single point or a series of points. curl https://api.opentopodata.org/v1/test-dataset?locations=56.35,123.90 { \"results\" : [{ \"elevation\" : 815.0 , \"location\" : { \"lat\" : 56.0 , \"lng\" : 123.0 }, \"dataset\" : \"test-dataset\" }], \"status\" : \"OK\" } The interpolation algorithm used can be configured as a request parameter, and locations can also be provided in Google Polyline format. See the API docs for more about request and response formats.","title":"Usage"},{"location":"#public-api","text":"I'm hosting a free public API at api.opentopodata.org . To keep the public API sustainable some limitations are applied. Max 100 locations per request. Max 1 call per second. Max 1000 calls per day. The following datasets are available on the public API, with elevation shown for downtown Denver, Colorado (39.7471, -104.9963). Dataset name Resolution Extent Source API link (Denver, CO) nzdem8m 8 m New Zealand. LINZ Not in dataset bounds ned10m ~10 m Continental USA, Hawaii, parts of Alaska. USGS 1590 m eudem25m 25 m Europe. EEA Not in dataset bounds mapzen ~30 m Global, inluding bathymetry. Mapzen 1590 m aster30m ~30 m Global. NASA 1591 m srtm30m ~30 m Latitudes -60 to 60. USGS 1604 m srtm90m ~90 m Latitudes -60 to 60. USGS 1603 m bkg200m 200 m Germany. BKG Not in dataset bounds etopo1 ~1.8 km Global, including bathymetry and ice surface elevation near poles. NOAA 1596 m gebco2020 ~450m Global bathymetry and land elevation. GEBCO 1603 m emod2018 ~100m Bathymetry for ocean and sea in Europe. EMODnet Not in dataset bounds See the API docs for more about request formats and parameters.","title":"Public API"},{"location":"#support","text":"Need some help getting Open Topo Data running? Send me an email at andrew@opentopodata.org !","title":"Support"},{"location":"#paid-hosting","text":"If you need an elevation API service with high-quality 1m lidar data, check out my sister project GPXZ . The GPXZ Elevation API offers the following features: Managed hosting, load balanced for redundancy Seamless, global, hi-res elevation dataset Drop-in replacement endpoint for the Google Maps Elevation API Priority support No hard usage limits EU-only servers if needed CORS (so you can use the API in your frontend webapp) For more details, reach out to andrew@opentopodata.org . Paid hosting funds the development of Open Topo Data and keeps the public API free.","title":"Paid hosting"},{"location":"api/","text":"API Documentation A public API is available for testing at api.opentopodata.org . GET /v1/<dataset_name> Reads the elevation from a given dataset. The dataset name must match one of the options in config.yaml . Multiple datasets can be provided separated by commas: in this case, for each point, each dataset is queried in order until a non-null elevation is found. For more information see Multi datasets . Latitudes and longitudes should be in EPSG:4326 (also known as WGS-84 format), they will be converted internally to whatever the dataset uses. Query Args locations : Required. Either latitutde,longitude pairs, each separated by a pipe character | . Example: locations=12.5,160.2|-10.6,130 . Google polyline format . Example: locations=gfo}EtohhU . samples : If provided, instead of using locations directly, query elevation for sample equally-spaced points along the path specified by locations . Example: samples=5 . interpolation : How to interpolate between the points in the dataset. Options: nearest , bilinear , cubic . Default: bilinear . nodata_value : What elevation to return if the dataset has a NODATA value at the requested location. Options: null , nan , or an integer like -9999 . Default: null . The default option null makes NODATA indistinguishable from a location outside the dataset bounds. NaN (not a number) values aren't valid in json and will break some clients. The nan option was default before version 1.4 and is provided only for backwards compatibility. When querying multiple datasets, this NODATA replacement only applies to the last dataset in the stack. format : Either json or geojson . Default: json . Response A json object, compatible with the Google Maps Elevation API. status : Will be OK for a successful request, INVALID_REQUEST for an input (4xx) error, and SERVER_ERROR for anything else (5xx). Required. error : Description of what went wrong, when status isn't OK . results : List of elevations for each location, in same order as input. Only provided for OK status. results[].elevation : Elevation, using units and datum from the dataset. Will be null if the given location is outside the dataset bounds. May be null for NODATA values depending on the nodata_value query argument. results[].location.lat : Latitude as parsed by Open Topo Data. results[].location.lng : Longitude as parsed by Open Topo Data. results[].dataset : The name of the dataset which the returned elevation is from. Some notes about the elevation value: If the raster has an integer data type, the interpolated elevation will be rounded to the nearest integer. This is a limitation of rasterio/gdal. If the request location isn't covered by any raster in the dataset, Open Topo Data will return null . Unless the nodata_value parameter is set, a null elevation could either mean the location is outside the dataset bounds, or a NODATA within the raster bounds. Example GET api.opentopodata.org/v1/srtm90m?locations=-43.5,172.5|27.6,1.98&interpolation=cubic { \"results\" : [ { \"dataset\" : \"srtm90m\" , \"elevation\" : 45 , \"location\" : { \"lat\" : -43.5 , \"lng\" : 172.5 } }, { \"dataset\" : \"srtm90m\" , \"elevation\" : 402 , \"location\" : { \"lat\" : 27.6 , \"lng\" : 1.98 } } ], \"status\" : \"OK\" } GeoJSON response If format=geojson is passed, you get a FeatureCollection of Point geometries instead. Each feature has its elevation as the z coordinate, and a dataset property specifying the source (corresponding to results[].dataset in the regular json response): GeoJSON example GET api.opentopodata.org/v1/srtm90m?locations=-43.5,172.5|27.6,1.98&interpolation=cubic&format=geojson { \"features\" : [ { \"geometry\" : { \"coordinates\" : [ 172.5 , -43.5 , 45 ], \"type\" : \"Point\" }, \"properties\" : { \"dataset\" : \"srtm90m\" }, \"type\" : \"Feature\" }, { \"geometry\" : { \"coordinates\" : [ 1.98 , 27.6 , 402 ], \"type\" : \"Point\" }, \"properties\" : { \"dataset\" : \"srtm90m\" }, \"type\" : \"Feature\" } ], \"type\" : \"FeatureCollection\" } POST /v1/<dataset_name> When querying many locations in a single request, you can run into issues fitting them all in one url. To avoid these issues, you can also send a POST request to /v1/<dataset_name> . The arguments are the same, but must be provided either as json-encoded data or form data instead of url query parameters. The response is the same. Other solutions for fitting many points in a URL are polyline encoding and rounding your coordinates. Example With json: import requests url = \"https://api.opentopodata.org/v1/srtm90m\" data = { \"locations\" : \"-43.5,172.5|27.6,1.98\" , \"interpolation\" : \"cubic\" , } response = requests . post ( url json = data ) With form data: import requests url = \"https://api.opentopodata.org/v1/srtm90m\" data = { \"locations\" : \"-43.5,172.5|27.6,1.98\" , \"interpolation\" : \"cubic\" , } response = requests . post ( url data = data ) The response is the same as for GET requests: { \"results\" : [ { \"dataset\" : \"srtm90m\" , \"elevation\" : 45 , \"location\" : { \"lat\" : -43.5 , \"lng\" : 172.5 } }, { \"dataset\" : \"srtm90m\" , \"elevation\" : 402 , \"location\" : { \"lat\" : 27.6 , \"lng\" : 1.98 } } ], \"status\" : \"OK\" } GET /health Healthcheck endpoint, for use with load balancing or monitoring. Response A json object. status : Will be OK for a successful request. The status code is 200 if healthy, otherwise 500. Example GET api.opentopodata.org/health { \"status\": \"OK\" } GET /datasets Details of the datasets available on the server. Response A json object. datasets : List of datasets. datasets[].name : Dataset name, used in the elevation query URL. datasets[].child_datasets : If the dataset is a MultiDataset, names of the child datasets. Otherwise, an empty list [] . status : Will be OK if the server is running and the config file can be loaded. Otherwise the value will be SERVER_ERROR . Example GET api.opentopodata.org/datasets { \"results\": [ { \"child_datasets\": [], \"name\": \"test-dataset\" } ] \"status\": \"OK\" }","title":"API docs"},{"location":"api/#api-documentation","text":"A public API is available for testing at api.opentopodata.org .","title":"API Documentation"},{"location":"api/#get-v1dataset_name","text":"Reads the elevation from a given dataset. The dataset name must match one of the options in config.yaml . Multiple datasets can be provided separated by commas: in this case, for each point, each dataset is queried in order until a non-null elevation is found. For more information see Multi datasets . Latitudes and longitudes should be in EPSG:4326 (also known as WGS-84 format), they will be converted internally to whatever the dataset uses.","title":"GET /v1/&lt;dataset_name&gt;"},{"location":"api/#query-args","text":"locations : Required. Either latitutde,longitude pairs, each separated by a pipe character | . Example: locations=12.5,160.2|-10.6,130 . Google polyline format . Example: locations=gfo}EtohhU . samples : If provided, instead of using locations directly, query elevation for sample equally-spaced points along the path specified by locations . Example: samples=5 . interpolation : How to interpolate between the points in the dataset. Options: nearest , bilinear , cubic . Default: bilinear . nodata_value : What elevation to return if the dataset has a NODATA value at the requested location. Options: null , nan , or an integer like -9999 . Default: null . The default option null makes NODATA indistinguishable from a location outside the dataset bounds. NaN (not a number) values aren't valid in json and will break some clients. The nan option was default before version 1.4 and is provided only for backwards compatibility. When querying multiple datasets, this NODATA replacement only applies to the last dataset in the stack. format : Either json or geojson . Default: json .","title":"Query Args"},{"location":"api/#response","text":"A json object, compatible with the Google Maps Elevation API. status : Will be OK for a successful request, INVALID_REQUEST for an input (4xx) error, and SERVER_ERROR for anything else (5xx). Required. error : Description of what went wrong, when status isn't OK . results : List of elevations for each location, in same order as input. Only provided for OK status. results[].elevation : Elevation, using units and datum from the dataset. Will be null if the given location is outside the dataset bounds. May be null for NODATA values depending on the nodata_value query argument. results[].location.lat : Latitude as parsed by Open Topo Data. results[].location.lng : Longitude as parsed by Open Topo Data. results[].dataset : The name of the dataset which the returned elevation is from. Some notes about the elevation value: If the raster has an integer data type, the interpolated elevation will be rounded to the nearest integer. This is a limitation of rasterio/gdal. If the request location isn't covered by any raster in the dataset, Open Topo Data will return null . Unless the nodata_value parameter is set, a null elevation could either mean the location is outside the dataset bounds, or a NODATA within the raster bounds.","title":"Response"},{"location":"api/#example","text":"GET api.opentopodata.org/v1/srtm90m?locations=-43.5,172.5|27.6,1.98&interpolation=cubic { \"results\" : [ { \"dataset\" : \"srtm90m\" , \"elevation\" : 45 , \"location\" : { \"lat\" : -43.5 , \"lng\" : 172.5 } }, { \"dataset\" : \"srtm90m\" , \"elevation\" : 402 , \"location\" : { \"lat\" : 27.6 , \"lng\" : 1.98 } } ], \"status\" : \"OK\" }","title":"Example"},{"location":"api/#geojson-response","text":"If format=geojson is passed, you get a FeatureCollection of Point geometries instead. Each feature has its elevation as the z coordinate, and a dataset property specifying the source (corresponding to results[].dataset in the regular json response):","title":"GeoJSON response"},{"location":"api/#geojson-example","text":"GET api.opentopodata.org/v1/srtm90m?locations=-43.5,172.5|27.6,1.98&interpolation=cubic&format=geojson { \"features\" : [ { \"geometry\" : { \"coordinates\" : [ 172.5 , -43.5 , 45 ], \"type\" : \"Point\" }, \"properties\" : { \"dataset\" : \"srtm90m\" }, \"type\" : \"Feature\" }, { \"geometry\" : { \"coordinates\" : [ 1.98 , 27.6 , 402 ], \"type\" : \"Point\" }, \"properties\" : { \"dataset\" : \"srtm90m\" }, \"type\" : \"Feature\" } ], \"type\" : \"FeatureCollection\" }","title":"GeoJSON example"},{"location":"api/#post-v1dataset_name","text":"When querying many locations in a single request, you can run into issues fitting them all in one url. To avoid these issues, you can also send a POST request to /v1/<dataset_name> . The arguments are the same, but must be provided either as json-encoded data or form data instead of url query parameters. The response is the same. Other solutions for fitting many points in a URL are polyline encoding and rounding your coordinates.","title":"POST /v1/&lt;dataset_name&gt;"},{"location":"api/#example_1","text":"With json: import requests url = \"https://api.opentopodata.org/v1/srtm90m\" data = { \"locations\" : \"-43.5,172.5|27.6,1.98\" , \"interpolation\" : \"cubic\" , } response = requests . post ( url json = data ) With form data: import requests url = \"https://api.opentopodata.org/v1/srtm90m\" data = { \"locations\" : \"-43.5,172.5|27.6,1.98\" , \"interpolation\" : \"cubic\" , } response = requests . post ( url data = data ) The response is the same as for GET requests: { \"results\" : [ { \"dataset\" : \"srtm90m\" , \"elevation\" : 45 , \"location\" : { \"lat\" : -43.5 , \"lng\" : 172.5 } }, { \"dataset\" : \"srtm90m\" , \"elevation\" : 402 , \"location\" : { \"lat\" : 27.6 , \"lng\" : 1.98 } } ], \"status\" : \"OK\" }","title":"Example"},{"location":"api/#get-health","text":"Healthcheck endpoint, for use with load balancing or monitoring.","title":"GET /health"},{"location":"api/#response_1","text":"A json object. status : Will be OK for a successful request. The status code is 200 if healthy, otherwise 500.","title":"Response"},{"location":"api/#example_2","text":"GET api.opentopodata.org/health { \"status\": \"OK\" }","title":"Example"},{"location":"api/#get-datasets","text":"Details of the datasets available on the server.","title":"GET /datasets"},{"location":"api/#response_2","text":"A json object. datasets : List of datasets. datasets[].name : Dataset name, used in the elevation query URL. datasets[].child_datasets : If the dataset is a MultiDataset, names of the child datasets. Otherwise, an empty list [] . status : Will be OK if the server is running and the config file can be loaded. Otherwise the value will be SERVER_ERROR .","title":"Response"},{"location":"api/#example_3","text":"GET api.opentopodata.org/datasets { \"results\": [ { \"child_datasets\": [], \"name\": \"test-dataset\" } ] \"status\": \"OK\" }","title":"Example"},{"location":"changelog/","text":"Release Notes This is a list of changes to Open Topo Data between each release. Version 1.10.0 (11 Oct 2024) Minior dependency upgrades Auto-reloading of config files without restarting docker ( #100 thanks @arnesetzer !) Version 1.9.0 (19 Feb 2024) Dependency upgrades, including python to 3.11 and rasterio to 1.3.9 Add support for geojson responses ( #86 , thanks @arnesetzer !) Fix handling of preflight requests ( #93 , thanks @MaRaSu !) Fix error message bug ( #70 , thanks @khendrickx !) Version 1.8.3 (7 Feb 2023) Fix memory leak ( #68 ) Fix invalid file error message ( #70 ) Downgrade to python 3.9 and rasterio 1.2.10 Version 1.8.2 (7 Nov 2022) Fix broken Docker image ( #66 ) Fix docs code highlighting Add Swisstopo documentation Get pylibmc from pypi instead of building from source (simplifying docker image) Upgrade uwsgi (to ease pathway to python 3.10) Siginficant dependency upgrades, including python to 3.10 Version 1.8.1 (26 Jul 2022) Better quoting in Makefile. Dependency updates including rasterio to 1.3.0, and patch upgrades to docker base images. Version 1.8.0 (4 Apr 2022) Added datasets endpoint. Add CONFIG_PATH environment variable. Minor dependency updates. Version 1.7.2 (16 Feb 2022) Instructions for running on kubernetes, thanks to @khintz . Updated main docker image from Python 3.7 to 3.9, and Debian from Buster to Bullseye. Minor dependency updates. Version 1.7.1 (13 Nov 2021) Support Apple Silicon M1 macs ( #55 ). Minor dependency updates. Version 1.7.0 (7 Oct 2021) Support POST requests ( #49 ). Dataset name can no longer be null for multi-dataset requests. Version 1.6.0 (7 Sep 2021) Added support for samples along a path ( #37 ). Added version response header ( #47 ). Enabled cors in example config file. Version 1.5.2 (17 Aug 2021) Updated dependencies. Version 1.5.1 (28 Apr 2021) Updated dependencies, including to rasterio 1.2.3. Fix some typos in scripts in the documentation. Version 1.5.0 (5 Feb 2021) Big performance improvements, thanks to caching expensive coordinate transforms, reducing the need to deserialise cached objects, and scaling processes to the machine being used. Add BKG data. Add Multiple Dataset feature. Updated rasterio and pyproj dependencies. Version 1.4.1 (10 Dec 2020) Increased max URI length. Support datasets with .prj files. Docs fixes. Small dependency updates. Version 1.4.0 (9 Nov 2020) Fixes bug #13 where responses could return invalid json. This changes the NODATA value from NaN to null . The old behaviour can be enabled by sending a nodata_value=nan query parameter. Small dependency updates. Version 1.3.1 (23 Oct 2020) Improved code style and test coverage. Updated dependencies. Documentation for running on Windows. Version 1.3.0 (4 Sep 2020) Added /health endpoint. Version 1.2.4 (8 Aug 2020) Support for more raster filename formats, as raised in issue #8 . Version 1.2.3 (31 July 2020) Minor documentation fixes and dependency updates. Version 1.2.2 (2 July 2020) Documentation fixes. I'm now using pip-tools to manage python dependencies, and I'm really liking it. Exact dependency versions are now pinned, but it's easy to update them to the latest version. Updated dependencies. Version 1.2.1 (22 May 2020) Improved documentation, plus some bug fixes: Fix floating-point precision issue that was breaking requests on dataset boundaries. Ignore common non-raster files. Version 1.2.0 (10 May 2020) Added a bunch of new datasets to the public API: GEBCO bathymetry. EMOD bathymetry. NZ 8m DEM. Mapzen 30m DEM. Version 1.1.0 (25 April 2020) Added this changelog! Pushing to master now means a release with a changelog entry. Added VERSION.txt, docker images get tagged with version when built. Documented install instructions and a brief overview for the datasets used in the public API. Makefile improvements (suggested by a user, thank you!). Increased the public API daily request limit to 1000. Updated NED on the public API. Added CORS header support.","title":"Release notes"},{"location":"changelog/#release-notes","text":"This is a list of changes to Open Topo Data between each release.","title":"Release Notes"},{"location":"changelog/#version-1100-11-oct-2024","text":"Minior dependency upgrades Auto-reloading of config files without restarting docker ( #100 thanks @arnesetzer !)","title":"Version 1.10.0 (11 Oct 2024)"},{"location":"changelog/#version-190-19-feb-2024","text":"Dependency upgrades, including python to 3.11 and rasterio to 1.3.9 Add support for geojson responses ( #86 , thanks @arnesetzer !) Fix handling of preflight requests ( #93 , thanks @MaRaSu !) Fix error message bug ( #70 , thanks @khendrickx !)","title":"Version 1.9.0 (19 Feb 2024)"},{"location":"changelog/#version-183-7-feb-2023","text":"Fix memory leak ( #68 ) Fix invalid file error message ( #70 ) Downgrade to python 3.9 and rasterio 1.2.10","title":"Version 1.8.3 (7 Feb 2023)"},{"location":"changelog/#version-182-7-nov-2022","text":"Fix broken Docker image ( #66 ) Fix docs code highlighting Add Swisstopo documentation Get pylibmc from pypi instead of building from source (simplifying docker image) Upgrade uwsgi (to ease pathway to python 3.10) Siginficant dependency upgrades, including python to 3.10","title":"Version 1.8.2 (7 Nov 2022)"},{"location":"changelog/#version-181-26-jul-2022","text":"Better quoting in Makefile. Dependency updates including rasterio to 1.3.0, and patch upgrades to docker base images.","title":"Version 1.8.1 (26 Jul 2022)"},{"location":"changelog/#version-180-4-apr-2022","text":"Added datasets endpoint. Add CONFIG_PATH environment variable. Minor dependency updates.","title":"Version 1.8.0 (4 Apr 2022)"},{"location":"changelog/#version-172-16-feb-2022","text":"Instructions for running on kubernetes, thanks to @khintz . Updated main docker image from Python 3.7 to 3.9, and Debian from Buster to Bullseye. Minor dependency updates.","title":"Version 1.7.2 (16 Feb 2022)"},{"location":"changelog/#version-171-13-nov-2021","text":"Support Apple Silicon M1 macs ( #55 ). Minor dependency updates.","title":"Version 1.7.1 (13 Nov 2021)"},{"location":"changelog/#version-170-7-oct-2021","text":"Support POST requests ( #49 ). Dataset name can no longer be null for multi-dataset requests.","title":"Version 1.7.0 (7 Oct 2021)"},{"location":"changelog/#version-160-7-sep-2021","text":"Added support for samples along a path ( #37 ). Added version response header ( #47 ). Enabled cors in example config file.","title":"Version 1.6.0 (7 Sep 2021)"},{"location":"changelog/#version-152-17-aug-2021","text":"Updated dependencies.","title":"Version 1.5.2 (17 Aug 2021)"},{"location":"changelog/#version-151-28-apr-2021","text":"Updated dependencies, including to rasterio 1.2.3. Fix some typos in scripts in the documentation.","title":"Version 1.5.1 (28 Apr 2021)"},{"location":"changelog/#version-150-5-feb-2021","text":"Big performance improvements, thanks to caching expensive coordinate transforms, reducing the need to deserialise cached objects, and scaling processes to the machine being used. Add BKG data. Add Multiple Dataset feature. Updated rasterio and pyproj dependencies.","title":"Version 1.5.0 (5 Feb 2021)"},{"location":"changelog/#version-141-10-dec-2020","text":"Increased max URI length. Support datasets with .prj files. Docs fixes. Small dependency updates.","title":"Version 1.4.1 (10 Dec 2020)"},{"location":"changelog/#version-140-9-nov-2020","text":"Fixes bug #13 where responses could return invalid json. This changes the NODATA value from NaN to null . The old behaviour can be enabled by sending a nodata_value=nan query parameter. Small dependency updates.","title":"Version 1.4.0 (9 Nov 2020)"},{"location":"changelog/#version-131-23-oct-2020","text":"Improved code style and test coverage. Updated dependencies. Documentation for running on Windows.","title":"Version 1.3.1 (23 Oct 2020)"},{"location":"changelog/#version-130-4-sep-2020","text":"Added /health endpoint.","title":"Version 1.3.0 (4 Sep 2020)"},{"location":"changelog/#version-124-8-aug-2020","text":"Support for more raster filename formats, as raised in issue #8 .","title":"Version 1.2.4 (8 Aug 2020)"},{"location":"changelog/#version-123-31-july-2020","text":"Minor documentation fixes and dependency updates.","title":"Version 1.2.3 (31 July 2020)"},{"location":"changelog/#version-122-2-july-2020","text":"Documentation fixes. I'm now using pip-tools to manage python dependencies, and I'm really liking it. Exact dependency versions are now pinned, but it's easy to update them to the latest version. Updated dependencies.","title":"Version 1.2.2 (2 July 2020)"},{"location":"changelog/#version-121-22-may-2020","text":"Improved documentation, plus some bug fixes: Fix floating-point precision issue that was breaking requests on dataset boundaries. Ignore common non-raster files.","title":"Version 1.2.1 (22 May 2020)"},{"location":"changelog/#version-120-10-may-2020","text":"Added a bunch of new datasets to the public API: GEBCO bathymetry. EMOD bathymetry. NZ 8m DEM. Mapzen 30m DEM.","title":"Version 1.2.0 (10 May 2020)"},{"location":"changelog/#version-110-25-april-2020","text":"Added this changelog! Pushing to master now means a release with a changelog entry. Added VERSION.txt, docker images get tagged with version when built. Documented install instructions and a brief overview for the datasets used in the public API. Makefile improvements (suggested by a user, thank you!). Increased the public API daily request limit to 1000. Updated NED on the public API. Added CORS header support.","title":"Version 1.1.0 (25 April 2020)"},{"location":"server/","text":"Open Topo Data Server Documentation Getting started The easiest way to run Open Topo Data is with Docker. Install docker then run the following commands: git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata make build make run This will start a server on localhost:5000 with a small demo dataset called test-dataset . Check out the API docs for info about the format of requests and responses. Getting started on M1 / Apple Silicon Macs On M1 Macs you'll probably need to use the alternate apple-silicon.Dockerfile docker image, which includes build dependencies for libraries that don't have binaries for M1. git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata make build-m1 make run This should work without rosetta. See issue #55 for more info. Getting started on Windows On Windows you'll probably need to run the build and run commands without make: git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata docker build --tag opentopodata --file docker/Dockerfile . docker run --rm -it --volume C:/path/to/opentopodata/data:/app/data:ro -p 5000 :5000 opentopodata sh -c \"/usr/bin/supervisord -c /app/docker/supervisord.conf\" For more details see this note on windows support . Dataset support Open Topo Data supports all georeferenced raster formats supported by GDAL (e.g, .tiff , .hgt , .jp2 ). Datasets can take one of two formats: A single raster file. A collection of square raster tiles which follow the SRTM naming convention: the file is named for the lower left corner. So a file named N30W120.tiff would span from 30 to 31 degrees latitude, and -120 to -119 degrees longitude. By default tiles are 1\u00b0 by 1\u00b0 and the coordinates are in WGS84, but this can be configured. If your dataset consists of multiple files that aren't on a nice grid, you can create a .vrt file pointing to the files that Open Topo Data will treat as a single-file dataset. For an example of this process, see the documentation for configuring EMODnet . Configuration Open Topo Data is configured by a config.yaml file. If that file is missing it will look for example-config.yaml instead. You can set the CONFIG_PATH environment variable to specify a different path. A config might look like: max_locations_per_request : 100 access_control_allow_origin : '*' datasets : - name : etopo1 path : data/etopo1/ - name : srtm90m path : data/srtm-90m-v3/ filename_epsg : 4326 filename_tile_size : 1 corresponding to a directory structure: opentopodata | \u2514\u2500\u2500\u2500data | \u251c\u2500\u2500\u2500etopo1 | | | \u2514\u2500\u2500\u2500etopo1-dem.geotiff | \u2514\u2500\u2500\u2500srtm-90m-v3 | \u251c\u2500\u2500\u2500N00E000.hgt \u251c\u2500\u2500\u2500N00E001.hgt \u251c\u2500\u2500\u2500N00E002.hgt \u251c\u2500\u2500\u2500etc... which would expose localhost:5000/v1/etopo1 and localhost:5000/v1/srtm90m . Mofifying the config file triggers a restart of OpenTopoData (which will reload the new config). Config spec max_locations_per_request : Requests with more than this many locations will return a 400 error. Default: 100 . access_control_allow_origin : Value for the Access-Control-Allow-Origin CORS header. Set to * or a domain to allow in-browser requests from a different origin. Set to null to send no Access-Control-Allow-Origin header. Default: null . datasets[].name : Dataset name, used in url. Required. datasets[].path : Path to folder containing the dataset. If the dataset is a single file it must be placed inside a folder. This path is relative to the repository directory inside docker. I suggest placing datasets inside the provided data folder, which is mounted in docker by make run . Files can be nested arbitrarily inside the dataset path. Required. datasets[].filename_epsg : For tiled datasets, the projection of the filename coordinates. The default value is 4326 , which is latitude/longitude with the WGS84 datum . datasets[].filename_tile_size : For tiled datasets, how large each square tile is in the units of filename_epsg . For example, a lat,lon location of 38.2,121.2 would lie in the tile N38W121 for a tile size of 1, but lie in N35W120 for a tile size of 5. For non-integer tile sizes like 2.5 , specify them as a string to avoid floating point parsing issues: \"2.5\" . Default: 1 . datasets[].wgs84_bounds.left : Leftmost (westmost) longitude of the dataset, in WGS84. Used as a performance optimisation for Multi datasets . Default: -180 . datasets[].wgs84_bounds.right : Rightmost (eastmost) longitude of the dataset. Default: 180 . datasets[].wgs84_bounds.bottom : Bottommost (southmost) latitude of the dataset. Default: -90 . datasets[].wgs84_bounds.top : Topmost (northmost) latitude of the dataset. Default: 90 . datasets[].child_datasets[] : A list of names of other datasets. Querying this MultiDataset will check each dataset in child_datasets in order until a non-null elevation is found. For more information see Multi datasets . Adding datasets An important goal of Open Topo Data is make is easy to add new datasets. The included dataset is very low resolution ( etopo1 downsampled to about 100km) and is intended only for testing. Adding a new dataset takes two steps: placing the dataset in the data directory adding the path to the dataset in config.yaml . Instructions are provided for adding the various datasets used in the public API: ASTER ETOPO1 EU-DEM Mapzen NED 10m NZ DEM SRTM (30m or 90m) EMOD Bathymetry GEBCO Bathymetry BKG Germany (200m) Kubernetes See How to deploy on Kubernetes for details and config files for running on kubernetes.","title":"Server docs"},{"location":"server/#open-topo-data-server-documentation","text":"","title":"Open Topo Data Server Documentation"},{"location":"server/#getting-started","text":"The easiest way to run Open Topo Data is with Docker. Install docker then run the following commands: git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata make build make run This will start a server on localhost:5000 with a small demo dataset called test-dataset . Check out the API docs for info about the format of requests and responses.","title":"Getting started"},{"location":"server/#getting-started-on-m1-apple-silicon-macs","text":"On M1 Macs you'll probably need to use the alternate apple-silicon.Dockerfile docker image, which includes build dependencies for libraries that don't have binaries for M1. git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata make build-m1 make run This should work without rosetta. See issue #55 for more info.","title":"Getting started on M1 / Apple Silicon Macs"},{"location":"server/#getting-started-on-windows","text":"On Windows you'll probably need to run the build and run commands without make: git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata docker build --tag opentopodata --file docker/Dockerfile . docker run --rm -it --volume C:/path/to/opentopodata/data:/app/data:ro -p 5000 :5000 opentopodata sh -c \"/usr/bin/supervisord -c /app/docker/supervisord.conf\" For more details see this note on windows support .","title":"Getting started on Windows"},{"location":"server/#dataset-support","text":"Open Topo Data supports all georeferenced raster formats supported by GDAL (e.g, .tiff , .hgt , .jp2 ). Datasets can take one of two formats: A single raster file. A collection of square raster tiles which follow the SRTM naming convention: the file is named for the lower left corner. So a file named N30W120.tiff would span from 30 to 31 degrees latitude, and -120 to -119 degrees longitude. By default tiles are 1\u00b0 by 1\u00b0 and the coordinates are in WGS84, but this can be configured. If your dataset consists of multiple files that aren't on a nice grid, you can create a .vrt file pointing to the files that Open Topo Data will treat as a single-file dataset. For an example of this process, see the documentation for configuring EMODnet .","title":"Dataset support"},{"location":"server/#configuration","text":"Open Topo Data is configured by a config.yaml file. If that file is missing it will look for example-config.yaml instead. You can set the CONFIG_PATH environment variable to specify a different path. A config might look like: max_locations_per_request : 100 access_control_allow_origin : '*' datasets : - name : etopo1 path : data/etopo1/ - name : srtm90m path : data/srtm-90m-v3/ filename_epsg : 4326 filename_tile_size : 1 corresponding to a directory structure: opentopodata | \u2514\u2500\u2500\u2500data | \u251c\u2500\u2500\u2500etopo1 | | | \u2514\u2500\u2500\u2500etopo1-dem.geotiff | \u2514\u2500\u2500\u2500srtm-90m-v3 | \u251c\u2500\u2500\u2500N00E000.hgt \u251c\u2500\u2500\u2500N00E001.hgt \u251c\u2500\u2500\u2500N00E002.hgt \u251c\u2500\u2500\u2500etc... which would expose localhost:5000/v1/etopo1 and localhost:5000/v1/srtm90m . Mofifying the config file triggers a restart of OpenTopoData (which will reload the new config).","title":"Configuration"},{"location":"server/#config-spec","text":"max_locations_per_request : Requests with more than this many locations will return a 400 error. Default: 100 . access_control_allow_origin : Value for the Access-Control-Allow-Origin CORS header. Set to * or a domain to allow in-browser requests from a different origin. Set to null to send no Access-Control-Allow-Origin header. Default: null . datasets[].name : Dataset name, used in url. Required. datasets[].path : Path to folder containing the dataset. If the dataset is a single file it must be placed inside a folder. This path is relative to the repository directory inside docker. I suggest placing datasets inside the provided data folder, which is mounted in docker by make run . Files can be nested arbitrarily inside the dataset path. Required. datasets[].filename_epsg : For tiled datasets, the projection of the filename coordinates. The default value is 4326 , which is latitude/longitude with the WGS84 datum . datasets[].filename_tile_size : For tiled datasets, how large each square tile is in the units of filename_epsg . For example, a lat,lon location of 38.2,121.2 would lie in the tile N38W121 for a tile size of 1, but lie in N35W120 for a tile size of 5. For non-integer tile sizes like 2.5 , specify them as a string to avoid floating point parsing issues: \"2.5\" . Default: 1 . datasets[].wgs84_bounds.left : Leftmost (westmost) longitude of the dataset, in WGS84. Used as a performance optimisation for Multi datasets . Default: -180 . datasets[].wgs84_bounds.right : Rightmost (eastmost) longitude of the dataset. Default: 180 . datasets[].wgs84_bounds.bottom : Bottommost (southmost) latitude of the dataset. Default: -90 . datasets[].wgs84_bounds.top : Topmost (northmost) latitude of the dataset. Default: 90 . datasets[].child_datasets[] : A list of names of other datasets. Querying this MultiDataset will check each dataset in child_datasets in order until a non-null elevation is found. For more information see Multi datasets .","title":"Config spec"},{"location":"server/#adding-datasets","text":"An important goal of Open Topo Data is make is easy to add new datasets. The included dataset is very low resolution ( etopo1 downsampled to about 100km) and is intended only for testing. Adding a new dataset takes two steps: placing the dataset in the data directory adding the path to the dataset in config.yaml . Instructions are provided for adding the various datasets used in the public API: ASTER ETOPO1 EU-DEM Mapzen NED 10m NZ DEM SRTM (30m or 90m) EMOD Bathymetry GEBCO Bathymetry BKG Germany (200m)","title":"Adding datasets"},{"location":"server/#kubernetes","text":"See How to deploy on Kubernetes for details and config files for running on kubernetes.","title":"Kubernetes"},{"location":"datasets/aster/","text":"ASTER Overview The Advanced Spaceborne Thermal Emission and Reflection Radiometer ( ASTER ) global DEM dataset is a joint effort between the Ministry of Economy, Trade, and Industry (METI) of Japan and the National Aeronautics and Space Administration (NASA) of the US. ASTER GDEM is a 1 arc-second resolution, corresponding to a resolution of about 30m at the equator. Coverage is provided from from -83 to 83 degrees latitude. Render of ASTER elevation. Source. Adding 30m ASTER to Open Topo Data Make a new folder for the dataset: mkdir ./data/aster30m Download the files from USGS into ./data/aster30m . Extract the zip archives keeping the _dem.tif files and removing the _num.tif files. To make downloading a bit easier, here's a list of the 22,912 URLs: aster30m_urls.txt . Create a config.yaml file: datasets : - name : aster30m path : data/aster30m/ Rebuild to enable the new dataset at localhost:5000/v1/aster30m . make build && make run Public API The Open Topo Data public API lets you query ASTER GDEM 30m for free: curl https://api.opentopodata.org/v1/aster30m?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 45.0 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"aster30m\" } ], \"status\" : \"OK\" } The Public API uses version 3 of the DEM (GDEM 003).","title":"ASTER"},{"location":"datasets/aster/#aster","text":"","title":"ASTER"},{"location":"datasets/aster/#overview","text":"The Advanced Spaceborne Thermal Emission and Reflection Radiometer ( ASTER ) global DEM dataset is a joint effort between the Ministry of Economy, Trade, and Industry (METI) of Japan and the National Aeronautics and Space Administration (NASA) of the US. ASTER GDEM is a 1 arc-second resolution, corresponding to a resolution of about 30m at the equator. Coverage is provided from from -83 to 83 degrees latitude. Render of ASTER elevation. Source.","title":"Overview"},{"location":"datasets/aster/#adding-30m-aster-to-open-topo-data","text":"Make a new folder for the dataset: mkdir ./data/aster30m Download the files from USGS into ./data/aster30m . Extract the zip archives keeping the _dem.tif files and removing the _num.tif files. To make downloading a bit easier, here's a list of the 22,912 URLs: aster30m_urls.txt . Create a config.yaml file: datasets : - name : aster30m path : data/aster30m/ Rebuild to enable the new dataset at localhost:5000/v1/aster30m . make build && make run","title":"Adding 30m ASTER to Open Topo Data"},{"location":"datasets/aster/#public-api","text":"The Open Topo Data public API lets you query ASTER GDEM 30m for free: curl https://api.opentopodata.org/v1/aster30m?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 45.0 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"aster30m\" } ], \"status\" : \"OK\" } The Public API uses version 3 of the DEM (GDEM 003).","title":"Public API"},{"location":"datasets/bkg/","text":"BKG BKG (Bundesamt f\u00fcr Kartographie und Geod\u00e4sie) has published a number of elevation datasets for Germany. The 200m and 1000m resolutions are freely available, while resolutions up to 5m can be purchased for a fee. Render of BKG 200m DTM elevation. Adding 200m BKG Digital Terrain Model to Open Topo Data Make a new folder for the dataset: mkdir ./data/bkg200m Download the dataset , the link here is for the UTM referenced dataset in .asc format. Extract the zip and copy only dgm200_utm32s.asc and dgm200_utm32s.prj into ./data/bkg200m . Add the dataset to config.yaml : - name : bkg200m path : data/bkg200m Finally, rebuild to enable the new dataset at localhost:5000/v1/bkg200m?locations=49.427,7.753 . make build && make run Adding 5m Germany DEM to Open Topo Data I don't have access to the higher-resolution datasets, but one user managed to get them working with Open Topo Data in issues #22 and #24 . The files come with a projection format that isn't supported by GDAL and with non-overlapping tiles so queries near the edges will return null data. Probably the easiest way to handle these issues is to merge all the files into one big geotiff: gdalbuildvrt -tap -a_srs epsg:3044 -o bkg-dgm5.vrt dgm5/*.asc gdaltranslate -co COMPRESS = DEFLATE -co BIGTIFF = YES -co NUM_THREADS = ALL_CPUS bkg-dgm5.vrt bkg-dgm5.tif You could also add a buffer to each tile, fixing the projection with -a_srs epsg:3044 . If anyone has got this working and would like to share their steps, please open an issue or pull request ! Public API The Open Topo Data public API lets you query 200m BKG DEM over Germany for free: curl https://api.opentopodata.org/v1/bkg200m?locations=49.427,7.753 { \"results\" : [ { \"elevation\" : 269.9404602050781 , \"location\" : { \"lat\" : 49.427 , \"lng\" : 7.753 }, \"dataset\" : \"bkg200m\" } ], \"status\" : \"OK\" } Attribution \u00a9 GeoBasis-DE / BKG (Jahr des Datenbezugs) Datenlizenz Deutschland \u2013 Namensnennung \u2013 Version 2.0","title":"BKG"},{"location":"datasets/bkg/#bkg","text":"BKG (Bundesamt f\u00fcr Kartographie und Geod\u00e4sie) has published a number of elevation datasets for Germany. The 200m and 1000m resolutions are freely available, while resolutions up to 5m can be purchased for a fee. Render of BKG 200m DTM elevation.","title":"BKG"},{"location":"datasets/bkg/#adding-200m-bkg-digital-terrain-model-to-open-topo-data","text":"Make a new folder for the dataset: mkdir ./data/bkg200m Download the dataset , the link here is for the UTM referenced dataset in .asc format. Extract the zip and copy only dgm200_utm32s.asc and dgm200_utm32s.prj into ./data/bkg200m . Add the dataset to config.yaml : - name : bkg200m path : data/bkg200m Finally, rebuild to enable the new dataset at localhost:5000/v1/bkg200m?locations=49.427,7.753 . make build && make run","title":"Adding 200m BKG Digital Terrain Model to Open Topo Data"},{"location":"datasets/bkg/#adding-5m-germany-dem-to-open-topo-data","text":"I don't have access to the higher-resolution datasets, but one user managed to get them working with Open Topo Data in issues #22 and #24 . The files come with a projection format that isn't supported by GDAL and with non-overlapping tiles so queries near the edges will return null data. Probably the easiest way to handle these issues is to merge all the files into one big geotiff: gdalbuildvrt -tap -a_srs epsg:3044 -o bkg-dgm5.vrt dgm5/*.asc gdaltranslate -co COMPRESS = DEFLATE -co BIGTIFF = YES -co NUM_THREADS = ALL_CPUS bkg-dgm5.vrt bkg-dgm5.tif You could also add a buffer to each tile, fixing the projection with -a_srs epsg:3044 . If anyone has got this working and would like to share their steps, please open an issue or pull request !","title":"Adding 5m Germany DEM to Open Topo Data"},{"location":"datasets/bkg/#public-api","text":"The Open Topo Data public API lets you query 200m BKG DEM over Germany for free: curl https://api.opentopodata.org/v1/bkg200m?locations=49.427,7.753 { \"results\" : [ { \"elevation\" : 269.9404602050781 , \"location\" : { \"lat\" : 49.427 , \"lng\" : 7.753 }, \"dataset\" : \"bkg200m\" } ], \"status\" : \"OK\" }","title":"Public API"},{"location":"datasets/bkg/#attribution","text":"\u00a9 GeoBasis-DE / BKG (Jahr des Datenbezugs) Datenlizenz Deutschland \u2013 Namensnennung \u2013 Version 2.0","title":"Attribution"},{"location":"datasets/emod2018/","text":"EMODnet 2018 Bathymetry EMODnet maintains a number of bathymetry (sea floor depth) datasets. There are currently two datasets with full coverage of Europe: a 1/8 arc minute (~200m) version released in 2016, and a 1/16 arc minute (~100m) version released in 2018. The datasets are a composite of bathymetric surveys, Landsat 8 imagery, and GEBCO data. The vertical datum used is Lowest Astronomical Tide . Coverage The 2018 dataset covers a large area around Europe: 15\u00b0 to 90\u00b0 latitude, and -36\u00b0 to 43\u00b0 longitude. The elevation over land areas is given as NODATA values. Render of EMODnet 2018 sea floor depth. Adding EMODnet 2018 Bathymetry to Open Topo Data Make a new folder for the dataset: mkdir ./data/emod2018 Download the files from EMODnet into ./data/emod2018 . You want the 2018 dataset, in ESRI ASCII format. Extract the zip folders, you should have 59 .asc named like A1_2018.asc . Unlike other datasets, the EMOD tiles aren't aligned on a nice whole-number grid, so Open Topo Data can't tell from the filenames which tile covers which area. To handle this we can build a https://gdal.org/drivers/raster/vrt.html - a single raster file that links to the 59 tiles and which Open Topo Data can treat as a single-file dataset. Unfortunately you'll need to install GDAL for this. Create a new folder for the VRT: mkdir ./data/emod2018-vrt Build the vrt using relative paths so the file will work inside the docker image: cd ./data/emod2018-vrt gdalbuildvrt -a_srs epsg:4326 emod2018.vrt ../emod2018/*.asc cd ../ Create a config.yaml file, pointing to the VRT folder: datasets : - name : emod2018 path : data/emod2018-vrt/ Rebuild to enable the new dataset at localhost:5000/v1/emod2018 . make build && make run Extra performance .asc files take up a lot of disk space and are slow for random reads. Consider converting to a compressed geotiff: gdal_translate -co COMPRESS=DEFLATE -a_srs epsg:4326 {asc_filename} {tif_filename} Public API The Open Topo Data public API lets you query NED 10m for free: curl https://api.opentopodata.org/v1/emod2018?locations=55.884323,2.528276 { \"results\" : [ { \"elevation\" : -81.94999694824219 , \"location\" : { \"lat\" : 55.884323 , \"lng\" : 2.528276 }, \"dataset\" : \"emod2018\" } ], \"status\" : \"OK\" } The public API uses the 2018 version of the dataset. Attribution EMODnet Bathymetry Consortium (2018): EMODnet Digital Bathymetry (DTM) .","title":"EMOD bathymetry"},{"location":"datasets/emod2018/#emodnet-2018-bathymetry","text":"EMODnet maintains a number of bathymetry (sea floor depth) datasets. There are currently two datasets with full coverage of Europe: a 1/8 arc minute (~200m) version released in 2016, and a 1/16 arc minute (~100m) version released in 2018. The datasets are a composite of bathymetric surveys, Landsat 8 imagery, and GEBCO data. The vertical datum used is Lowest Astronomical Tide .","title":"EMODnet 2018 Bathymetry"},{"location":"datasets/emod2018/#coverage","text":"The 2018 dataset covers a large area around Europe: 15\u00b0 to 90\u00b0 latitude, and -36\u00b0 to 43\u00b0 longitude. The elevation over land areas is given as NODATA values. Render of EMODnet 2018 sea floor depth.","title":"Coverage"},{"location":"datasets/emod2018/#adding-emodnet-2018-bathymetry-to-open-topo-data","text":"Make a new folder for the dataset: mkdir ./data/emod2018 Download the files from EMODnet into ./data/emod2018 . You want the 2018 dataset, in ESRI ASCII format. Extract the zip folders, you should have 59 .asc named like A1_2018.asc . Unlike other datasets, the EMOD tiles aren't aligned on a nice whole-number grid, so Open Topo Data can't tell from the filenames which tile covers which area. To handle this we can build a https://gdal.org/drivers/raster/vrt.html - a single raster file that links to the 59 tiles and which Open Topo Data can treat as a single-file dataset. Unfortunately you'll need to install GDAL for this. Create a new folder for the VRT: mkdir ./data/emod2018-vrt Build the vrt using relative paths so the file will work inside the docker image: cd ./data/emod2018-vrt gdalbuildvrt -a_srs epsg:4326 emod2018.vrt ../emod2018/*.asc cd ../ Create a config.yaml file, pointing to the VRT folder: datasets : - name : emod2018 path : data/emod2018-vrt/ Rebuild to enable the new dataset at localhost:5000/v1/emod2018 . make build && make run Extra performance .asc files take up a lot of disk space and are slow for random reads. Consider converting to a compressed geotiff: gdal_translate -co COMPRESS=DEFLATE -a_srs epsg:4326 {asc_filename} {tif_filename}","title":"Adding EMODnet 2018 Bathymetry to Open Topo Data"},{"location":"datasets/emod2018/#public-api","text":"The Open Topo Data public API lets you query NED 10m for free: curl https://api.opentopodata.org/v1/emod2018?locations=55.884323,2.528276 { \"results\" : [ { \"elevation\" : -81.94999694824219 , \"location\" : { \"lat\" : 55.884323 , \"lng\" : 2.528276 }, \"dataset\" : \"emod2018\" } ], \"status\" : \"OK\" } The public API uses the 2018 version of the dataset.","title":"Public API"},{"location":"datasets/emod2018/#attribution","text":"EMODnet Bathymetry Consortium (2018): EMODnet Digital Bathymetry (DTM) .","title":"Attribution"},{"location":"datasets/etopo1/","text":"ETOPO1 Overview ETOPO1 is a global elevation dataset developed by NOAA. Unlike many DEM datasets, ETOPO1 contains bathymetry (water depth). There are two variants of the dataset, which vary in how elevation is calculated for the Antartic and Greenland ice sheets: an ice-surface variant, and a bedrock-level variant. The dataset has a 1 arc-minute resolution, which corresponds to a resolution of about 1.8km at the equator. ETOPO1 was made by aggregating many other datasets. The bulk of the land elevation comes from SRTM30, while most bathymetry is sourced from GEBCO . The comprising datasets were normalised to the same vertical datum (sea level) and horizontal datum (WGS84). Accuracy The accuracy of ETOPO1 varies spatially depending on the underlying source data. NOAA estimates the vertical accuracy is no better than 10m. The quality of ETOPO1 is high: there are no missing values or holes (holes in the SRTM30 source were fixed by hand). Transitions between source datasets are smooth. Pseudocolour render of ETOPO1 elevation. Adding ETOPO1 to Open Topo Data Download the grid-registered .tif file from noaa.gov to the data directory and unzip. mkdir ./data/etopo1 wget -P ./data/etopo1 https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/ice_surface/grid_registered/georeferenced_tiff/ETOPO1_Ice_g_geotiff.zip unzip ./data/etopo1/ETOPO1_Ice_g_geotiff.zip rm ./data/etopo1/ETOPO1_Ice_g_geotiff.zip The provided .tif file doesn't include projection information, which is needed for Open Topo Data. It can be added with GDAL: gdal_translate -a_srs EPSG:4326 ./data/etopo1/ETOPO1_Ice_g_geotiff.tif ./data/etopo1/ETOPO1.tif rm ./data/etopo1/ETOPO1_Ice_g_geotiff.tif Create a file config.yaml with the following contents datasets : - name : etopo1 path : data/etopo1/ Rebuild to enable the new dataset at localhost:5000/v1/etopo1?locations=27.98,86.92 make build && make run Public API The Open Topo Data public API lets you query ETOPO1 for free: curl https://api.opentopodata.org/v1/etopo1?locations=39.747114,-104.996334 { \"results\" : [ { \"elevation\" : 1596.0 , \"location\" : { \"lat\" : 39.747114 , \"lng\" : -104.996334 }, \"dataset\" : \"etopo1\" } ], \"status\" : \"OK\" } Open Topo Data hosts the ice-elevation version of the dataset (the same as seen in the image above). Attribution doi:10.7289/V5C8276M","title":"ETOPO1"},{"location":"datasets/etopo1/#etopo1","text":"","title":"ETOPO1"},{"location":"datasets/etopo1/#overview","text":"ETOPO1 is a global elevation dataset developed by NOAA. Unlike many DEM datasets, ETOPO1 contains bathymetry (water depth). There are two variants of the dataset, which vary in how elevation is calculated for the Antartic and Greenland ice sheets: an ice-surface variant, and a bedrock-level variant. The dataset has a 1 arc-minute resolution, which corresponds to a resolution of about 1.8km at the equator. ETOPO1 was made by aggregating many other datasets. The bulk of the land elevation comes from SRTM30, while most bathymetry is sourced from GEBCO . The comprising datasets were normalised to the same vertical datum (sea level) and horizontal datum (WGS84).","title":"Overview"},{"location":"datasets/etopo1/#accuracy","text":"The accuracy of ETOPO1 varies spatially depending on the underlying source data. NOAA estimates the vertical accuracy is no better than 10m. The quality of ETOPO1 is high: there are no missing values or holes (holes in the SRTM30 source were fixed by hand). Transitions between source datasets are smooth. Pseudocolour render of ETOPO1 elevation.","title":"Accuracy"},{"location":"datasets/etopo1/#adding-etopo1-to-open-topo-data","text":"Download the grid-registered .tif file from noaa.gov to the data directory and unzip. mkdir ./data/etopo1 wget -P ./data/etopo1 https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/ice_surface/grid_registered/georeferenced_tiff/ETOPO1_Ice_g_geotiff.zip unzip ./data/etopo1/ETOPO1_Ice_g_geotiff.zip rm ./data/etopo1/ETOPO1_Ice_g_geotiff.zip The provided .tif file doesn't include projection information, which is needed for Open Topo Data. It can be added with GDAL: gdal_translate -a_srs EPSG:4326 ./data/etopo1/ETOPO1_Ice_g_geotiff.tif ./data/etopo1/ETOPO1.tif rm ./data/etopo1/ETOPO1_Ice_g_geotiff.tif Create a file config.yaml with the following contents datasets : - name : etopo1 path : data/etopo1/ Rebuild to enable the new dataset at localhost:5000/v1/etopo1?locations=27.98,86.92 make build && make run","title":"Adding ETOPO1 to Open Topo Data"},{"location":"datasets/etopo1/#public-api","text":"The Open Topo Data public API lets you query ETOPO1 for free: curl https://api.opentopodata.org/v1/etopo1?locations=39.747114,-104.996334 { \"results\" : [ { \"elevation\" : 1596.0 , \"location\" : { \"lat\" : 39.747114 , \"lng\" : -104.996334 }, \"dataset\" : \"etopo1\" } ], \"status\" : \"OK\" } Open Topo Data hosts the ice-elevation version of the dataset (the same as seen in the image above).","title":"Public API"},{"location":"datasets/etopo1/#attribution","text":"doi:10.7289/V5C8276M","title":"Attribution"},{"location":"datasets/eudem/","text":"EU-DEM EU-DEM is an elevation dataset covering Europe at a 25 metre resolution. The dataset was created by merging elevation data from the SRTM and ASTER global datasets, as well as from Soviet topo maps at high latitudes. The datum used is EVRS2000 . Coverage The dataset covers European Environment Agency member states, plus some countries to the east. Coverage extends to small parts of Northern Africa. Unlike SRTM, EU-DEM includes the Scandinavian regions north of 60\u00b0. Render of EU-DEM elevation. Accuracy The stated vertical accuracy is \u00b1 7m RMSE. Differences to SRTM and ASTER typically fall within this 7m range even with the datasets using slightly different vertical datums. Elevations for Lake Geneva, Switzerland are 370m , 374m , and 372m for SRTM, ASTER, and EU-DEM respectively. Coastline EU-DEM uses NODATA values for elevations over seas and oceans, where both ASTER and SRTM assign these areas an elevation of 0m. This means that Open Topo Data isn't able to interpolate elevations for locations very close to the coast and will return a value of NaN in places where SRTM and ASTER might return a 0m or 1m elevation. The advantage of the NODATA oceans is that you cane use EU-DEM without clipping to a coastline shapefile. Adding EU-DEM to Open Topo Data As of Jan 2024, EU-DEM is no longer available to download via copernicus.eu. I have uploaded my version of the dataset at https://files.gpxz.io/eudem_buffered.zip , see EUDEM download for more details. Download and unzip the folder into: mkdir ./data/eudem There are 27 files. Then create a config.yaml file: datasets : - name : eudem25m path : data/eudem filename_epsg : 3035 filename_tile_size : 1000000 Finally, rebuild to enable the new dataset at localhost:5000/v1/eudem25m?locations=51.575,-3.220 . make build && make run Avoiding gdal If you don't have gdal installed, you can use the tiles directly. There are instructions for this here , but because the EU-DEM tiles don't come with an overlap you will get a null elevation at locations within 0.5 pixels of tile edges. Public API The Open Topo Data public API lets you query EU-DEM for free: curl https://api.opentopodata.org/v1/eudem25m?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 54.576168060302734 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"eudem25m\" } ], \"status\" : \"OK\" } Open Topo Data hosts version 1.1 of the dataset. Attribution Released by Copernicus under the following terms: Access to data is based on a principle of full, open and free access as established by the Copernicus data and information policy Regulation (EU) No 1159/2013 of 12 July 2013. This regulation establishes registration and licensing conditions for GMES/Copernicus users. Free, full and open access to this data set is made on the conditions that: When distributing or communicating Copernicus dedicated data and Copernicus service information to the public, users shall inform the public of the source of that data and information. Users shall make sure not to convey the impression to the public that the user's activities are officially endorsed by the Union. Where that data or information has been adapted or modified, the user shall clearly state this. The data remain the sole property of the European Union. Any information and data produced in the framework of the action shall be the sole property of the European Union. Any communication and publication by the beneficiary shall acknowledge that the data were produced \"with funding by the European Union\".","title":"EU-DEM"},{"location":"datasets/eudem/#eu-dem","text":"EU-DEM is an elevation dataset covering Europe at a 25 metre resolution. The dataset was created by merging elevation data from the SRTM and ASTER global datasets, as well as from Soviet topo maps at high latitudes. The datum used is EVRS2000 .","title":"EU-DEM"},{"location":"datasets/eudem/#coverage","text":"The dataset covers European Environment Agency member states, plus some countries to the east. Coverage extends to small parts of Northern Africa. Unlike SRTM, EU-DEM includes the Scandinavian regions north of 60\u00b0. Render of EU-DEM elevation.","title":"Coverage"},{"location":"datasets/eudem/#accuracy","text":"The stated vertical accuracy is \u00b1 7m RMSE. Differences to SRTM and ASTER typically fall within this 7m range even with the datasets using slightly different vertical datums. Elevations for Lake Geneva, Switzerland are 370m , 374m , and 372m for SRTM, ASTER, and EU-DEM respectively.","title":"Accuracy"},{"location":"datasets/eudem/#coastline","text":"EU-DEM uses NODATA values for elevations over seas and oceans, where both ASTER and SRTM assign these areas an elevation of 0m. This means that Open Topo Data isn't able to interpolate elevations for locations very close to the coast and will return a value of NaN in places where SRTM and ASTER might return a 0m or 1m elevation. The advantage of the NODATA oceans is that you cane use EU-DEM without clipping to a coastline shapefile.","title":"Coastline"},{"location":"datasets/eudem/#adding-eu-dem-to-open-topo-data","text":"As of Jan 2024, EU-DEM is no longer available to download via copernicus.eu. I have uploaded my version of the dataset at https://files.gpxz.io/eudem_buffered.zip , see EUDEM download for more details. Download and unzip the folder into: mkdir ./data/eudem There are 27 files. Then create a config.yaml file: datasets : - name : eudem25m path : data/eudem filename_epsg : 3035 filename_tile_size : 1000000 Finally, rebuild to enable the new dataset at localhost:5000/v1/eudem25m?locations=51.575,-3.220 . make build && make run Avoiding gdal If you don't have gdal installed, you can use the tiles directly. There are instructions for this here , but because the EU-DEM tiles don't come with an overlap you will get a null elevation at locations within 0.5 pixels of tile edges.","title":"Adding EU-DEM to Open Topo Data"},{"location":"datasets/eudem/#public-api","text":"The Open Topo Data public API lets you query EU-DEM for free: curl https://api.opentopodata.org/v1/eudem25m?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 54.576168060302734 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"eudem25m\" } ], \"status\" : \"OK\" } Open Topo Data hosts version 1.1 of the dataset.","title":"Public API"},{"location":"datasets/eudem/#attribution","text":"Released by Copernicus under the following terms: Access to data is based on a principle of full, open and free access as established by the Copernicus data and information policy Regulation (EU) No 1159/2013 of 12 July 2013. This regulation establishes registration and licensing conditions for GMES/Copernicus users. Free, full and open access to this data set is made on the conditions that: When distributing or communicating Copernicus dedicated data and Copernicus service information to the public, users shall inform the public of the source of that data and information. Users shall make sure not to convey the impression to the public that the user's activities are officially endorsed by the Union. Where that data or information has been adapted or modified, the user shall clearly state this. The data remain the sole property of the European Union. Any information and data produced in the framework of the action shall be the sole property of the European Union. Any communication and publication by the beneficiary shall acknowledge that the data were produced \"with funding by the European Union\".","title":"Attribution"},{"location":"datasets/gebco2020/","text":"GEBCO 2020 Bathymetry GEBCO maintains a high-quality, global bathymetry (sea floor depth) dataset. GEBCO releases a new dataset most years, the 2020 dataset (released in May 2020) covers the entire globe at a 15 arc-second resolution, corresponding to 450m resolution at the equator. Coverage Elevation is given for land areas, largely using a 15-degree version of SRTM . Seafloor data comes from a variety of bathymetric sources, see GEBCO for more details. Render of GEBCO 2020 elevation. Adding GEBCO 2020 to Open Topo Data Instructions are given for the 2020 version of the dataset: future versions might work a bit differently. Make a new folder for the dataset: mkdir ./data/gebco2020 Download the dataset from GEBCO . You'll want the GEBCO_2020 Grid version, in Data GeoTiff format. Extract raster tiles from the archive and delete everything else so there are just 8 .tif files in the ./data/gebco2020 folder. The files are given as 90 degree tiles, we need to rename them to SRTM's NxxSxx format to work with Open Topo Data: mv gebco_2020_n0.0_s-90.0_w0.0_e90.0.tif S90E000.tif mv gebco_2020_n0.0_s-90.0_w-180.0_e-90.0.tif S90W180.tif mv gebco_2020_n0.0_s-90.0_w-90.0_e0.0.tif S90W090.tif mv gebco_2020_n0.0_s-90.0_w90.0_e180.0.tif S90E090.tif mv gebco_2020_n90.0_s0.0_w0.0_e90.0.tif N00E000.tif mv gebco_2020_n90.0_s0.0_w-180.0_e-90.0.tif N00W180.tif mv gebco_2020_n90.0_s0.0_w-90.0_e0.0.tif N00W090.tif mv gebco_2020_n90.0_s0.0_w90.0_e180.0.tif N00E090.tif Create a config.yaml file: datasets : - name : gebco2020 path : data/gebco2020/ filename_tile_size : 90 Rebuild to enable the new dataset at localhost:5000/v1/gebco2020 . make build && make run Buffering tiles The tiles provided by GEBCO don't overlap and cover slightly less than a 90\u00b0 x 90\u00b0 square. This means you'll get a null result for coordinates along the tile edges (like 0,0 ). For the public API I used the following code to add a 5px buffer to each tile. from glob import glob import os import rasterio old_folder = 'gebco_2020_geotiff' new_folder = 'gebco_2020_buffer' buffer_ = 5 old_pattern = os . path . join ( old_folder , '*.tif' ) old_paths = list ( glob ( old_pattern )) cmd = 'gdalbuildvrt {} /all.vrt' . format ( old_folder ) + ' ' . join ( old_paths ) os . system ( cmd ) for path in old_paths : new_path = path . replace ( old_folder , new_folder ) with rasterio . open ( path ) as f : new_bounds = ( f . bounds . left - buffer_ * f . res [ 0 ], f . bounds . bottom - buffer_ * f . res [ 1 ], f . bounds . right + buffer_ * f . res [ 0 ], f . bounds . top + buffer_ * f . res [ 1 ], ) new_shape = ( f . shape [ 0 ] + buffer_ * 2 , f . shape [ 1 ] + buffer_ * 2 , ) te = ' ' . join ( str ( x ) for x in new_bounds ) ts = ' ' . join ( str ( x ) for x in new_shape ) cmd = f 'gdalwarp -te { te } -ts { ts } -r near -co NUM_THREADS=ALL_CPUS -co COMPRESS=DEFLATE -co PREDICTOR=2 -co BIGTIFF=yes { old_folder } /all.vrt { new_path } ' os . system ( cmd ) Public API The Open Topo Data public API lets you query GEBCO 2020 for free: curl https://api.opentopodata.org/v1/gebco2020?locations=37.6535,-119.4105 { \"results\" : [ { \"elevation\" : 3405.0 , \"location\" : { \"lat\" : 37.6535 , \"lng\" : -119.4105 }, \"dataset\" : \"gebco2020\" } ], \"status\" : \"OK\" } The public API uses the 2020 version of the dataset. Attribution GEBCO released the dataset into the public domain under the following terms: Acknowledge the source of The GEBCO Grid. A suitable form of attribution is given in the documentation that accompanies The GEBCO Grid. Not use The GEBCO Grid in a way that suggests any official status or that GEBCO, or the IHO or IOC, endorses any particular application of The GEBCO Grid. Not mislead others or misrepresent The GEBCO Grid or its source.","title":"GEBCO bathymetry"},{"location":"datasets/gebco2020/#gebco-2020-bathymetry","text":"GEBCO maintains a high-quality, global bathymetry (sea floor depth) dataset. GEBCO releases a new dataset most years, the 2020 dataset (released in May 2020) covers the entire globe at a 15 arc-second resolution, corresponding to 450m resolution at the equator.","title":"GEBCO 2020 Bathymetry"},{"location":"datasets/gebco2020/#coverage","text":"Elevation is given for land areas, largely using a 15-degree version of SRTM . Seafloor data comes from a variety of bathymetric sources, see GEBCO for more details. Render of GEBCO 2020 elevation.","title":"Coverage"},{"location":"datasets/gebco2020/#adding-gebco-2020-to-open-topo-data","text":"Instructions are given for the 2020 version of the dataset: future versions might work a bit differently. Make a new folder for the dataset: mkdir ./data/gebco2020 Download the dataset from GEBCO . You'll want the GEBCO_2020 Grid version, in Data GeoTiff format. Extract raster tiles from the archive and delete everything else so there are just 8 .tif files in the ./data/gebco2020 folder. The files are given as 90 degree tiles, we need to rename them to SRTM's NxxSxx format to work with Open Topo Data: mv gebco_2020_n0.0_s-90.0_w0.0_e90.0.tif S90E000.tif mv gebco_2020_n0.0_s-90.0_w-180.0_e-90.0.tif S90W180.tif mv gebco_2020_n0.0_s-90.0_w-90.0_e0.0.tif S90W090.tif mv gebco_2020_n0.0_s-90.0_w90.0_e180.0.tif S90E090.tif mv gebco_2020_n90.0_s0.0_w0.0_e90.0.tif N00E000.tif mv gebco_2020_n90.0_s0.0_w-180.0_e-90.0.tif N00W180.tif mv gebco_2020_n90.0_s0.0_w-90.0_e0.0.tif N00W090.tif mv gebco_2020_n90.0_s0.0_w90.0_e180.0.tif N00E090.tif Create a config.yaml file: datasets : - name : gebco2020 path : data/gebco2020/ filename_tile_size : 90 Rebuild to enable the new dataset at localhost:5000/v1/gebco2020 . make build && make run","title":"Adding GEBCO 2020 to Open Topo Data"},{"location":"datasets/gebco2020/#buffering-tiles","text":"The tiles provided by GEBCO don't overlap and cover slightly less than a 90\u00b0 x 90\u00b0 square. This means you'll get a null result for coordinates along the tile edges (like 0,0 ). For the public API I used the following code to add a 5px buffer to each tile. from glob import glob import os import rasterio old_folder = 'gebco_2020_geotiff' new_folder = 'gebco_2020_buffer' buffer_ = 5 old_pattern = os . path . join ( old_folder , '*.tif' ) old_paths = list ( glob ( old_pattern )) cmd = 'gdalbuildvrt {} /all.vrt' . format ( old_folder ) + ' ' . join ( old_paths ) os . system ( cmd ) for path in old_paths : new_path = path . replace ( old_folder , new_folder ) with rasterio . open ( path ) as f : new_bounds = ( f . bounds . left - buffer_ * f . res [ 0 ], f . bounds . bottom - buffer_ * f . res [ 1 ], f . bounds . right + buffer_ * f . res [ 0 ], f . bounds . top + buffer_ * f . res [ 1 ], ) new_shape = ( f . shape [ 0 ] + buffer_ * 2 , f . shape [ 1 ] + buffer_ * 2 , ) te = ' ' . join ( str ( x ) for x in new_bounds ) ts = ' ' . join ( str ( x ) for x in new_shape ) cmd = f 'gdalwarp -te { te } -ts { ts } -r near -co NUM_THREADS=ALL_CPUS -co COMPRESS=DEFLATE -co PREDICTOR=2 -co BIGTIFF=yes { old_folder } /all.vrt { new_path } ' os . system ( cmd )","title":"Buffering tiles"},{"location":"datasets/gebco2020/#public-api","text":"The Open Topo Data public API lets you query GEBCO 2020 for free: curl https://api.opentopodata.org/v1/gebco2020?locations=37.6535,-119.4105 { \"results\" : [ { \"elevation\" : 3405.0 , \"location\" : { \"lat\" : 37.6535 , \"lng\" : -119.4105 }, \"dataset\" : \"gebco2020\" } ], \"status\" : \"OK\" } The public API uses the 2020 version of the dataset.","title":"Public API"},{"location":"datasets/gebco2020/#attribution","text":"GEBCO released the dataset into the public domain under the following terms: Acknowledge the source of The GEBCO Grid. A suitable form of attribution is given in the documentation that accompanies The GEBCO Grid. Not use The GEBCO Grid in a way that suggests any official status or that GEBCO, or the IHO or IOC, endorses any particular application of The GEBCO Grid. Not mislead others or misrepresent The GEBCO Grid or its source.","title":"Attribution"},{"location":"datasets/mapzen/","text":"Mapzen Mapzen's terrain tiles are a global DEM dataset, including bathymetry. The dataset is an assimilation of multiple open datasets . Coverage and resolution Data is provided at a 1 arc-second resolution, corresponding to a resolution of about 30m at the equator. However, parts of the dataset are interpolated from lower-resolution datasets. The resolution of the source datasets is shown below: Resolution of Mapzen source datasets. Source: github.com/tilezen/joerd . Adding Mapzen to Open Topo Data Make a new folder for the dataset: mkdir ./data/mapzen Download the tiles from AWS. I found it easiest to use the aws cli : aws s3 cp --no-sign-request --recursive s3://elevation-tiles-prod/skadi ./data/mapzen Extract all the .hgt files. Create a config.yaml file: datasets : - name : mapzen path : data/mapzen/ Rebuild to enable the new dataset at localhost:5000/v1/mapzen . make build && make run Extra performance .hgt files are extremely large. You'll get a large space reduction with little read penalty by converting to a compressed geotiff: gdal_translate -co COMPRESS=DEFLATE -co PREDICTOR=2 {hgt_filename} {tif_filename} Public API The Open Topo Data public API lets you query the Mapzen dataset for free: curl https://api.opentopodata.org/v1/mapzen?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 55.0 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"mapzen\" } ], \"status\" : \"OK\" } The public API uses Version 1.1 of Mapzen, downloaded from AWS on May 2020","title":"Mapzen"},{"location":"datasets/mapzen/#mapzen","text":"Mapzen's terrain tiles are a global DEM dataset, including bathymetry. The dataset is an assimilation of multiple open datasets .","title":"Mapzen"},{"location":"datasets/mapzen/#coverage-and-resolution","text":"Data is provided at a 1 arc-second resolution, corresponding to a resolution of about 30m at the equator. However, parts of the dataset are interpolated from lower-resolution datasets. The resolution of the source datasets is shown below: Resolution of Mapzen source datasets. Source: github.com/tilezen/joerd .","title":"Coverage and resolution"},{"location":"datasets/mapzen/#adding-mapzen-to-open-topo-data","text":"Make a new folder for the dataset: mkdir ./data/mapzen Download the tiles from AWS. I found it easiest to use the aws cli : aws s3 cp --no-sign-request --recursive s3://elevation-tiles-prod/skadi ./data/mapzen Extract all the .hgt files. Create a config.yaml file: datasets : - name : mapzen path : data/mapzen/ Rebuild to enable the new dataset at localhost:5000/v1/mapzen . make build && make run Extra performance .hgt files are extremely large. You'll get a large space reduction with little read penalty by converting to a compressed geotiff: gdal_translate -co COMPRESS=DEFLATE -co PREDICTOR=2 {hgt_filename} {tif_filename}","title":"Adding Mapzen to Open Topo Data"},{"location":"datasets/mapzen/#public-api","text":"The Open Topo Data public API lets you query the Mapzen dataset for free: curl https://api.opentopodata.org/v1/mapzen?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 55.0 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"mapzen\" } ], \"status\" : \"OK\" } The public API uses Version 1.1 of Mapzen, downloaded from AWS on May 2020","title":"Public API"},{"location":"datasets/ned/","text":"NED The National Elevation Dataset (NED) is a collection of DEMs covering the USA at different resolutions. Resolution and Coverage NED comes in several different resolutions, each with a different coverage area. The most commonly used resolutions are 1 arcsecond (covering North America and Mexico) and 1/3 arcsecond (covering CONUS, HI, PR, and parts of AK). The 1/3 arcsecond dataset is used in the Open Topo Data public API. 1 arcsecond (30m). 1/3 arcsecond (10m). Two higher resolutions have partial coverage focused on more urbanised areas. 1/9 arcsecond (3m). 1m. And there are separate datasets with full coverage of Alaska at 2 arseconds (60m) and 5m. 2 arcsecond (60m). 5m. Coverage screenshots are from The National Map . Adding NED 10m to Open Topo Data Make a new folder for the dataset: mkdir ./data/ned10m Download the files from USGS into ./data/ned10m . You want the USGS_13_xxxxxxx.tif files. Next, Open Topo Data needs the filenames to match the SRTM format: the filename should be the coordinates of the lower-left corner. Here's the Python code I used to do the conversion. from glob import glob import os import re old_pattern = './data/ned10m/USGS_13_*.tif' old_paths = list ( glob ( old_pattern )) print ( 'Found {} files' . format ( len ( old_paths ))) for old_path in old_paths : folder = os . path . dirname ( old_path ) old_filename = os . path . basename ( old_path ) # Extract northing. res = re . search ( r '([ns]\\d\\d)' , old_filename ) old_northing = res . groups ()[ 0 ] # Fix the NS n_or_s = old_northing [ 0 ] ns_value = int ( old_northing [ 1 : 3 ]) if old_northing [: 3 ] == 'n00' : new_northing = 's01' + old_northing [ 3 :] elif n_or_s == 'n' : new_northing = 'n' + str ( ns_value - 1 ) . zfill ( 2 ) + old_northing [ 3 :] elif n_or_s == 's' : new_northing = 's' + str ( ns_value + 1 ) . zfill ( 2 ) + old_northing [ 3 :] new_filename = old_filename . replace ( old_northing , new_northing ) assert new_northing in new_filename # Prevent new filename from overwriting old tiles. parts = new_filename . split ( '.' ) parts [ 0 ] = parts [ 0 ] + '_renamed' new_filename = '.' . join ( parts ) # Rename in place. new_path = os . path . join ( folder , new_filename ) os . rename ( old_path , new_path ) Create a config.yaml file: datasets : - name : ned10m path : data/ned10m/ filename_epsg : 4269 Rebuild to enable the new dataset at localhost:5000/v1/ned10m . make build && make run Public API The Open Topo Data public API lets you query NED 10m for free: curl https://api.opentopodata.org/v1/ned10m?locations=37.6535,-119.4105 { \"results\" : [ { \"elevation\" : 3498.298583984375 , \"location\" : { \"lat\" : 37.6535 , \"lng\" : -119.4105 }, \"dataset\" : \"ned10m\" } ], \"status\" : \"OK\" } NED is still being updated by USGS. The dataset used by the public API was last updated 2020-04-23.","title":"NED"},{"location":"datasets/ned/#ned","text":"The National Elevation Dataset (NED) is a collection of DEMs covering the USA at different resolutions.","title":"NED"},{"location":"datasets/ned/#resolution-and-coverage","text":"NED comes in several different resolutions, each with a different coverage area. The most commonly used resolutions are 1 arcsecond (covering North America and Mexico) and 1/3 arcsecond (covering CONUS, HI, PR, and parts of AK). The 1/3 arcsecond dataset is used in the Open Topo Data public API. 1 arcsecond (30m). 1/3 arcsecond (10m). Two higher resolutions have partial coverage focused on more urbanised areas. 1/9 arcsecond (3m). 1m. And there are separate datasets with full coverage of Alaska at 2 arseconds (60m) and 5m. 2 arcsecond (60m). 5m. Coverage screenshots are from The National Map .","title":"Resolution and Coverage"},{"location":"datasets/ned/#adding-ned-10m-to-open-topo-data","text":"Make a new folder for the dataset: mkdir ./data/ned10m Download the files from USGS into ./data/ned10m . You want the USGS_13_xxxxxxx.tif files. Next, Open Topo Data needs the filenames to match the SRTM format: the filename should be the coordinates of the lower-left corner. Here's the Python code I used to do the conversion. from glob import glob import os import re old_pattern = './data/ned10m/USGS_13_*.tif' old_paths = list ( glob ( old_pattern )) print ( 'Found {} files' . format ( len ( old_paths ))) for old_path in old_paths : folder = os . path . dirname ( old_path ) old_filename = os . path . basename ( old_path ) # Extract northing. res = re . search ( r '([ns]\\d\\d)' , old_filename ) old_northing = res . groups ()[ 0 ] # Fix the NS n_or_s = old_northing [ 0 ] ns_value = int ( old_northing [ 1 : 3 ]) if old_northing [: 3 ] == 'n00' : new_northing = 's01' + old_northing [ 3 :] elif n_or_s == 'n' : new_northing = 'n' + str ( ns_value - 1 ) . zfill ( 2 ) + old_northing [ 3 :] elif n_or_s == 's' : new_northing = 's' + str ( ns_value + 1 ) . zfill ( 2 ) + old_northing [ 3 :] new_filename = old_filename . replace ( old_northing , new_northing ) assert new_northing in new_filename # Prevent new filename from overwriting old tiles. parts = new_filename . split ( '.' ) parts [ 0 ] = parts [ 0 ] + '_renamed' new_filename = '.' . join ( parts ) # Rename in place. new_path = os . path . join ( folder , new_filename ) os . rename ( old_path , new_path ) Create a config.yaml file: datasets : - name : ned10m path : data/ned10m/ filename_epsg : 4269 Rebuild to enable the new dataset at localhost:5000/v1/ned10m . make build && make run","title":"Adding NED 10m to Open Topo Data"},{"location":"datasets/ned/#public-api","text":"The Open Topo Data public API lets you query NED 10m for free: curl https://api.opentopodata.org/v1/ned10m?locations=37.6535,-119.4105 { \"results\" : [ { \"elevation\" : 3498.298583984375 , \"location\" : { \"lat\" : 37.6535 , \"lng\" : -119.4105 }, \"dataset\" : \"ned10m\" } ], \"status\" : \"OK\" } NED is still being updated by USGS. The dataset used by the public API was last updated 2020-04-23.","title":"Public API"},{"location":"datasets/nzdem/","text":"NZ DEM The 8m NZ DEM is an interpolation of the 20m contours on the 1:50,000 scale LINZ topo maps . Coverage The dataset covers all of New Zealand except Chatham Island at an 8 metre resolution. NZ DEM elevation rendering. Adding NZ DEM to Open Topo Data mkdir ./data/nzdem8m As of May 2020, the 8m dataset could only be painstaking downloaded a single tile at a time through the LINZ web interface . If you'd rather not do this, here are all the files as I downloaded them from LINZ on May 2020 nzdem-may-2020.zip . Once you've obtained the 115 files, unzip the zip archives and delete anything without a .tif extension. For Open Topo Data to understand the grid arrangement of the files, they need to be renamed to the coordinates of the lower-left corner. Here's the Python script I used, I'm also adding a buffer to help with interpolation near tile borders: import os from glob import glob folder = './data/nzdem8m' # Build vrt for all tifs. pattern = os . path . join ( folder , '*.tif' ) tif_paths = list ( glob ( pattern )) vrt_path = os . path . join ( folder , 'all.vrt' ) assert not os . system ( 'gdalbuildvrt {} {} ' . format ( vrt_path , ' ' . join ( tif_paths ))) buffer_ = 5 for tif_path in tif_paths : with rasterio . open ( tif_path ) as f : new_bounds = ( f . bounds . left - buffer_ * f . res [ 0 ], f . bounds . bottom - buffer_ * f . res [ 1 ], f . bounds . right + buffer_ * f . res [ 0 ], f . bounds . top + buffer_ * f . res [ 1 ], ) new_shape = ( f . shape [ 0 ] + buffer_ * 2 , f . shape [ 1 ] + buffer_ * 2 , ) northing = f . bounds . bottom easting = f . bounds . left filename = 'N {} E {} .tif' . format ( int ( northing ), int ( easting )) buffer_path = os . path . join ( os . path . dirname ( tif_path ), filename ) te = ' ' . join ( str ( x ) for x in new_bounds ) ts = ' ' . join ( str ( x ) for x in new_shape ) cmd = f 'gdalwarp -te { te } -ts { ts } -r near -co NUM_THREADS=ALL_CPUS -co COMPRESS=DEFLATE -co PREDICTOR=3 { vrt_path } { buffer_path } ' assert not os . system ( cmd ) assert not os . system ( f 'rm { vrt_path } ' ) Create a config.yaml file, setting the size of the tiles (65536 metres) and the projection system used ( NZ tranverse mercator ): datasets : - name : nzdem8m path : data/nzdem8m/ filename_tile_size : 65536 filename_epsg : 2193 Rebuild to enable the new dataset at localhost:5000/v1/nzdem8m . make build && make run Public API The Open Topo Data public API lets you query NZ DEM 8m for free: curl https://api.opentopodata.org/v1/nzdem8m?locations=-37.86118,174.79974 { \"results\" : [ { \"elevation\" : 705.4374389648438 , \"location\" : { \"lat\" : -37.86118 , \"lng\" : 174.79974 }, \"dataset\" : \"nzdem8m\" } ], \"status\" : \"OK\" } The data files used in the public API were downloaded from LINZ May 2020. Attribution Released by LINZ under the (https://creativecommons.org/licenses/by/4.0/)[Creative Commons Attribution 4.0 International] licence.","title":"NZ DEM"},{"location":"datasets/nzdem/#nz-dem","text":"The 8m NZ DEM is an interpolation of the 20m contours on the 1:50,000 scale LINZ topo maps .","title":"NZ DEM"},{"location":"datasets/nzdem/#coverage","text":"The dataset covers all of New Zealand except Chatham Island at an 8 metre resolution. NZ DEM elevation rendering.","title":"Coverage"},{"location":"datasets/nzdem/#adding-nz-dem-to-open-topo-data","text":"mkdir ./data/nzdem8m As of May 2020, the 8m dataset could only be painstaking downloaded a single tile at a time through the LINZ web interface . If you'd rather not do this, here are all the files as I downloaded them from LINZ on May 2020 nzdem-may-2020.zip . Once you've obtained the 115 files, unzip the zip archives and delete anything without a .tif extension. For Open Topo Data to understand the grid arrangement of the files, they need to be renamed to the coordinates of the lower-left corner. Here's the Python script I used, I'm also adding a buffer to help with interpolation near tile borders: import os from glob import glob folder = './data/nzdem8m' # Build vrt for all tifs. pattern = os . path . join ( folder , '*.tif' ) tif_paths = list ( glob ( pattern )) vrt_path = os . path . join ( folder , 'all.vrt' ) assert not os . system ( 'gdalbuildvrt {} {} ' . format ( vrt_path , ' ' . join ( tif_paths ))) buffer_ = 5 for tif_path in tif_paths : with rasterio . open ( tif_path ) as f : new_bounds = ( f . bounds . left - buffer_ * f . res [ 0 ], f . bounds . bottom - buffer_ * f . res [ 1 ], f . bounds . right + buffer_ * f . res [ 0 ], f . bounds . top + buffer_ * f . res [ 1 ], ) new_shape = ( f . shape [ 0 ] + buffer_ * 2 , f . shape [ 1 ] + buffer_ * 2 , ) northing = f . bounds . bottom easting = f . bounds . left filename = 'N {} E {} .tif' . format ( int ( northing ), int ( easting )) buffer_path = os . path . join ( os . path . dirname ( tif_path ), filename ) te = ' ' . join ( str ( x ) for x in new_bounds ) ts = ' ' . join ( str ( x ) for x in new_shape ) cmd = f 'gdalwarp -te { te } -ts { ts } -r near -co NUM_THREADS=ALL_CPUS -co COMPRESS=DEFLATE -co PREDICTOR=3 { vrt_path } { buffer_path } ' assert not os . system ( cmd ) assert not os . system ( f 'rm { vrt_path } ' ) Create a config.yaml file, setting the size of the tiles (65536 metres) and the projection system used ( NZ tranverse mercator ): datasets : - name : nzdem8m path : data/nzdem8m/ filename_tile_size : 65536 filename_epsg : 2193 Rebuild to enable the new dataset at localhost:5000/v1/nzdem8m . make build && make run","title":"Adding NZ DEM to Open Topo Data"},{"location":"datasets/nzdem/#public-api","text":"The Open Topo Data public API lets you query NZ DEM 8m for free: curl https://api.opentopodata.org/v1/nzdem8m?locations=-37.86118,174.79974 { \"results\" : [ { \"elevation\" : 705.4374389648438 , \"location\" : { \"lat\" : -37.86118 , \"lng\" : 174.79974 }, \"dataset\" : \"nzdem8m\" } ], \"status\" : \"OK\" } The data files used in the public API were downloaded from LINZ May 2020.","title":"Public API"},{"location":"datasets/nzdem/#attribution","text":"Released by LINZ under the (https://creativecommons.org/licenses/by/4.0/)[Creative Commons Attribution 4.0 International] licence.","title":"Attribution"},{"location":"datasets/srtm/","text":"SRTM Overview SRTM is a near-global elevation dataset, with coverage from -60 to 60 degrees latitude. SRTM comes in multiple resolutions. The highest resolution is 1 arc-second, which corresponds to a resolution of about 30m at the equator. The 3 arc-second (90m) version is also frequently used. Coverage SRTM has coverage from -60 to 60 degrees latitude. The dataset is released in 1 degree tiles. Ocean areas covered by a tile have an elevation of 0m. Open Topo Data will return null for locations not covered by a tile. SRTM coverage (green area). Downloading 30m SRTM Make a new folder for the dataset: mkdir ./data/srtm30m Download the files from USGS into ./data/srtm30m . Before downloading you'll need to register an account at earthdata.nasa.gov . Using these credentials for downloading is a little tricky, but luckily Earthdata provide download scripts in multiple different languages, the Python ones worked well for me. You want the xxxxxxx.SRTMGL1.hgt.zip files. To make downloading a bit easier, here's a list of the 14,297 URLs: srtm30m_urls.txt . If those scripts aren't working for you, an @SamDurand ( #70 ) had success with logging into Earthdata in your browser, then automating the browser to download the files: import webbrowser import time with open ( \"srtm30m_urls.txt\" , \"r\" ) as f : url_list = f . read () . split ( \" \\n \" ) for i , url in enumerate ( url_list ): webbrowser . open_new_tab ( url ) if i % 100 == 0 : time . sleep ( 5 ) # pause 5s every 100 it to avoid rate limiting. Adding 30m SRTM to Open Topo Data Create a config.yaml file: datasets : - name : srtm30m path : data/srtm30m/ Rebuild to enable the new dataset at localhost:5000/v1/srtm30m . make build && make run Extra performance .hgt.zip files are extremely slow for random reads. I got a 10x read speedup and a 10% size reduction from converting to a compressed geotiff: gdal_translate -co COMPRESS=DEFLATE -co PREDICTOR=2 {hgtzip_filename} {tif_filename} Unsupported file format If you're leaving the tiles in .hgt.zip format, be aware that 16 of the files are not able to be read by gdal. There are instructions for fixing those zip files . Adding 90m SRTM to Open Topo Data The process is the same as for 30m. The dataset is hosted on USGS here , and a list of the tile urls is here: srtm90m_urls.txt . Public API The Open Topo Data public API lets you query SRTM 30m for free: curl https://api.opentopodata.org/v1/srtm30m?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 55.0 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"srtm30m\" } ], \"status\" : \"OK\" } as well as SRTM 90m: curl https://api.opentopodata.org/v1/srtm90m?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 55.0 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"srtm90m\" } ], \"status\" : \"OK\" } The public API uses Version 3 of SRTM for both resolutions.","title":"SRTM"},{"location":"datasets/srtm/#srtm","text":"","title":"SRTM"},{"location":"datasets/srtm/#overview","text":"SRTM is a near-global elevation dataset, with coverage from -60 to 60 degrees latitude. SRTM comes in multiple resolutions. The highest resolution is 1 arc-second, which corresponds to a resolution of about 30m at the equator. The 3 arc-second (90m) version is also frequently used.","title":"Overview"},{"location":"datasets/srtm/#coverage","text":"SRTM has coverage from -60 to 60 degrees latitude. The dataset is released in 1 degree tiles. Ocean areas covered by a tile have an elevation of 0m. Open Topo Data will return null for locations not covered by a tile. SRTM coverage (green area).","title":"Coverage"},{"location":"datasets/srtm/#downloading-30m-srtm","text":"Make a new folder for the dataset: mkdir ./data/srtm30m Download the files from USGS into ./data/srtm30m . Before downloading you'll need to register an account at earthdata.nasa.gov . Using these credentials for downloading is a little tricky, but luckily Earthdata provide download scripts in multiple different languages, the Python ones worked well for me. You want the xxxxxxx.SRTMGL1.hgt.zip files. To make downloading a bit easier, here's a list of the 14,297 URLs: srtm30m_urls.txt . If those scripts aren't working for you, an @SamDurand ( #70 ) had success with logging into Earthdata in your browser, then automating the browser to download the files: import webbrowser import time with open ( \"srtm30m_urls.txt\" , \"r\" ) as f : url_list = f . read () . split ( \" \\n \" ) for i , url in enumerate ( url_list ): webbrowser . open_new_tab ( url ) if i % 100 == 0 : time . sleep ( 5 ) # pause 5s every 100 it to avoid rate limiting.","title":"Downloading 30m SRTM"},{"location":"datasets/srtm/#adding-30m-srtm-to-open-topo-data","text":"Create a config.yaml file: datasets : - name : srtm30m path : data/srtm30m/ Rebuild to enable the new dataset at localhost:5000/v1/srtm30m . make build && make run Extra performance .hgt.zip files are extremely slow for random reads. I got a 10x read speedup and a 10% size reduction from converting to a compressed geotiff: gdal_translate -co COMPRESS=DEFLATE -co PREDICTOR=2 {hgtzip_filename} {tif_filename} Unsupported file format If you're leaving the tiles in .hgt.zip format, be aware that 16 of the files are not able to be read by gdal. There are instructions for fixing those zip files .","title":"Adding 30m SRTM to Open Topo Data"},{"location":"datasets/srtm/#adding-90m-srtm-to-open-topo-data","text":"The process is the same as for 30m. The dataset is hosted on USGS here , and a list of the tile urls is here: srtm90m_urls.txt .","title":"Adding 90m SRTM to Open Topo Data"},{"location":"datasets/srtm/#public-api","text":"The Open Topo Data public API lets you query SRTM 30m for free: curl https://api.opentopodata.org/v1/srtm30m?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 55.0 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"srtm30m\" } ], \"status\" : \"OK\" } as well as SRTM 90m: curl https://api.opentopodata.org/v1/srtm90m?locations=57.688709,11.976404 { \"results\" : [ { \"elevation\" : 55.0 , \"location\" : { \"lat\" : 57.688709 , \"lng\" : 11.976404 }, \"dataset\" : \"srtm90m\" } ], \"status\" : \"OK\" } The public API uses Version 3 of SRTM for both resolutions.","title":"Public API"},{"location":"datasets/swisstopo/","text":"Swisstopo The Swisstopo Federal Office of Topography publishes a number of datasets, including very high quality elevation data for the whole of Switzerland at 0.5m and 2m resolutions under the name swissALTI . The data is regularly updated. Adding 2m Swiss DEM to Open Topo Data (The procedure is the same for the 0.5m product as the tiles are the same 1km extent). Make a new folder for the dataset: mkdir ./data/swisstopo-2m Download the urls of each tile from swisstopo . When I did this in 2021 there were 43,579 tiles. Download each tile. wget can do this. Consider doing the downloads sequentially with a rate-limit to avoid overloading their servers. wget --input-file ch.swisstopo.swissalti3d-xxxxx.csv --no-clobber --limit-rate 1M Open Topo Data needs files to be named with the lower-left corner coordinates. The files from swisstopo are named like this, but in km instead of metres. I use the following Python script to take a file named like swissalti3d_2019_2622-1264_2_2056_5728.tif and rename it to swissalti3d_2019_2622-1264_2_2056_5728.N1264000E2622000.tif from glob import glob import os dataset_folder = './data/swisstopo-2m/' pattern = os . path . join ( dataset_folder , '**' , '*.tif' ) paths = sorted ( glob ( pattern , recursive = True )) for old_path in paths : # Extract lower-left corder from filename. old_filename = os . path . basename ( old_path ) extent = old_filename . split ( '_' )[ 2 ] northing = extent . split ( '-' )[ 1 ] easting = extent . split ( '-' )[ 0 ] # Convert from km to m. northing = int ( northing ) * 1000 easting = int ( easting ) * 1000 # Build new filename. new_filename = old_filename . rsplit ( '.' , 1 )[ 0 ] new_filename = new_filename + f '.N { northing } E { easting } .tif' new_path = os . path . join ( os . path . dirname ( old_path ), new_filename ) # Rename the file. os . rename ( old_path , new_path ) The final step is adding the to config.yaml : - name : swisstopo-2m path : data/swisstopo-2m/ filename_epsg : 2056 filename_tile_size : 1000 Public API The Open Topo Data public server is full to the brim with elevation data at the moment! Swisstopo is first on the waiting list to be added once I upgrade the server. 2m Swiss elevation data is included in GPXZ Attribution \u00a9swisstopo","title":"Swisstopo"},{"location":"datasets/swisstopo/#swisstopo","text":"The Swisstopo Federal Office of Topography publishes a number of datasets, including very high quality elevation data for the whole of Switzerland at 0.5m and 2m resolutions under the name swissALTI . The data is regularly updated.","title":"Swisstopo"},{"location":"datasets/swisstopo/#adding-2m-swiss-dem-to-open-topo-data","text":"(The procedure is the same for the 0.5m product as the tiles are the same 1km extent). Make a new folder for the dataset: mkdir ./data/swisstopo-2m Download the urls of each tile from swisstopo . When I did this in 2021 there were 43,579 tiles. Download each tile. wget can do this. Consider doing the downloads sequentially with a rate-limit to avoid overloading their servers. wget --input-file ch.swisstopo.swissalti3d-xxxxx.csv --no-clobber --limit-rate 1M Open Topo Data needs files to be named with the lower-left corner coordinates. The files from swisstopo are named like this, but in km instead of metres. I use the following Python script to take a file named like swissalti3d_2019_2622-1264_2_2056_5728.tif and rename it to swissalti3d_2019_2622-1264_2_2056_5728.N1264000E2622000.tif from glob import glob import os dataset_folder = './data/swisstopo-2m/' pattern = os . path . join ( dataset_folder , '**' , '*.tif' ) paths = sorted ( glob ( pattern , recursive = True )) for old_path in paths : # Extract lower-left corder from filename. old_filename = os . path . basename ( old_path ) extent = old_filename . split ( '_' )[ 2 ] northing = extent . split ( '-' )[ 1 ] easting = extent . split ( '-' )[ 0 ] # Convert from km to m. northing = int ( northing ) * 1000 easting = int ( easting ) * 1000 # Build new filename. new_filename = old_filename . rsplit ( '.' , 1 )[ 0 ] new_filename = new_filename + f '.N { northing } E { easting } .tif' new_path = os . path . join ( os . path . dirname ( old_path ), new_filename ) # Rename the file. os . rename ( old_path , new_path ) The final step is adding the to config.yaml : - name : swisstopo-2m path : data/swisstopo-2m/ filename_epsg : 2056 filename_tile_size : 1000","title":"Adding 2m Swiss DEM to Open Topo Data"},{"location":"datasets/swisstopo/#public-api","text":"The Open Topo Data public server is full to the brim with elevation data at the moment! Swisstopo is first on the waiting list to be added once I upgrade the server. 2m Swiss elevation data is included in GPXZ","title":"Public API"},{"location":"datasets/swisstopo/#attribution","text":"\u00a9swisstopo","title":"Attribution"},{"location":"notes/buffering-tiles/","text":"Adding a Buffer to Tiles Most tiled elevation datasets have an overlap: a 1 pixel overlap lets you interpolate locations between pixels at the edge, and some datasets (like those produced by USGS) have larger datasets that give more flexibility with interpolation and resampling. However not all datasets come with an overlap, which makes it impossible to interpolate locations within 1 pixel of the edge of a tile without opening multiple files. Because Open Topo Data only opens one file for each location, it may return null for these border locations in non-overlapping datasets. This isn't a problem for everyone: if your tiles are large enough, only a very small fraction of pixels lie on the edges so you may never notice issues. But to fix this problem you can add a buffer to the tile files. I'll use EU-DEM as an example, as the tiles by default have no overlap. Start by downloading the 27 .TIF files into ./data/eudem . You'll also need to install the gdal commandline tools. Next we'll make a VRT file for the rasters mkdir ./data/eudem-vrt cd ./data/eudem-vrt gdalbuildvrt -tr 25 25 -tap -te 0 0 8000000 6000000 -co VRT_SHARED_SOURCE = 0 eudem.vrt ../eudem/*.TIF cd ../../ The tr , tap , and te options in the above command ensure that slices from the VRT will use the exact values and grid of the source rasters. Now we could actually stop here: if we add the eudem-vrt as a single-file dataset, Open Topo Data will handle the boundaries just fine. But for a slight performance improvement we can slice the VRT back into it's original tiles with an overlap. The following code will make put buffered tiles into data/eudem-buffered : import os from glob import glob import subprocess import rasterio # Prepare paths. input_pattern = 'data/eudem/*.TIF' input_paths = sorted ( glob ( input_pattern )) assert input_paths vrt_path = 'data/eudem-vrt/eudem.vrt' output_dir = 'data/eudem-buffered/' os . makedirs ( output_dir , exist_ok = True ) # EU-DEM specific options. tile_size = 1_000_000 buffer_size = 25 for input_path in input_paths : # Get tile bounds. with rasterio . open ( input_path ) as f : bottom = int ( f . bounds . bottom ) left = int ( f . bounds . left ) # For EU-DEM only: round this partial tile down to the nearest tile_size. if left == 943750 : left = 0 # New tile name in SRTM format. output_name = 'N' + str ( bottom ) . zfill ( 7 ) + 'E' + str ( left ) . zfill ( 7 ) + '.TIF' output_path = os . path . join ( output_dir , output_name ) # New bounds. xmin = left - buffer_size xmax = left + tile_size + buffer_size ymin = bottom - buffer_size ymax = bottom + tile_size + buffer_size # EU-DEM tiles don't cover negative locations. xmin = max ( 0 , xmin ) ymin = max ( 0 , ymin ) # Do the transformation. cmd = [ 'gdal_translate' , '-a_srs' , 'EPSG:3035' , # EU-DEM crs. '-co' , 'NUM_THREADS=ALL_CPUS' , '-co' , 'COMPRESS=DEFLATE' , '-co' , 'BIGTIFF=YES' , '--config' , 'GDAL_CACHEMAX' , '512' , '-projwin' , str ( xmin ), str ( ymax ), str ( xmax ), str ( ymin ), vrt_path , output_path , ] r = subprocess . run ( cmd ) r . check_returncode () These new files can be used in Open Topo Data with the following config.yaml file datasets : - name : eudem25m path : data/eudem-buffered/ filename_epsg : 3035 filename_tile_size : 1000000 after rebuilding: make build && make run","title":"Buffering tiles"},{"location":"notes/buffering-tiles/#adding-a-buffer-to-tiles","text":"Most tiled elevation datasets have an overlap: a 1 pixel overlap lets you interpolate locations between pixels at the edge, and some datasets (like those produced by USGS) have larger datasets that give more flexibility with interpolation and resampling. However not all datasets come with an overlap, which makes it impossible to interpolate locations within 1 pixel of the edge of a tile without opening multiple files. Because Open Topo Data only opens one file for each location, it may return null for these border locations in non-overlapping datasets. This isn't a problem for everyone: if your tiles are large enough, only a very small fraction of pixels lie on the edges so you may never notice issues. But to fix this problem you can add a buffer to the tile files. I'll use EU-DEM as an example, as the tiles by default have no overlap. Start by downloading the 27 .TIF files into ./data/eudem . You'll also need to install the gdal commandline tools. Next we'll make a VRT file for the rasters mkdir ./data/eudem-vrt cd ./data/eudem-vrt gdalbuildvrt -tr 25 25 -tap -te 0 0 8000000 6000000 -co VRT_SHARED_SOURCE = 0 eudem.vrt ../eudem/*.TIF cd ../../ The tr , tap , and te options in the above command ensure that slices from the VRT will use the exact values and grid of the source rasters. Now we could actually stop here: if we add the eudem-vrt as a single-file dataset, Open Topo Data will handle the boundaries just fine. But for a slight performance improvement we can slice the VRT back into it's original tiles with an overlap. The following code will make put buffered tiles into data/eudem-buffered : import os from glob import glob import subprocess import rasterio # Prepare paths. input_pattern = 'data/eudem/*.TIF' input_paths = sorted ( glob ( input_pattern )) assert input_paths vrt_path = 'data/eudem-vrt/eudem.vrt' output_dir = 'data/eudem-buffered/' os . makedirs ( output_dir , exist_ok = True ) # EU-DEM specific options. tile_size = 1_000_000 buffer_size = 25 for input_path in input_paths : # Get tile bounds. with rasterio . open ( input_path ) as f : bottom = int ( f . bounds . bottom ) left = int ( f . bounds . left ) # For EU-DEM only: round this partial tile down to the nearest tile_size. if left == 943750 : left = 0 # New tile name in SRTM format. output_name = 'N' + str ( bottom ) . zfill ( 7 ) + 'E' + str ( left ) . zfill ( 7 ) + '.TIF' output_path = os . path . join ( output_dir , output_name ) # New bounds. xmin = left - buffer_size xmax = left + tile_size + buffer_size ymin = bottom - buffer_size ymax = bottom + tile_size + buffer_size # EU-DEM tiles don't cover negative locations. xmin = max ( 0 , xmin ) ymin = max ( 0 , ymin ) # Do the transformation. cmd = [ 'gdal_translate' , '-a_srs' , 'EPSG:3035' , # EU-DEM crs. '-co' , 'NUM_THREADS=ALL_CPUS' , '-co' , 'COMPRESS=DEFLATE' , '-co' , 'BIGTIFF=YES' , '--config' , 'GDAL_CACHEMAX' , '512' , '-projwin' , str ( xmin ), str ( ymax ), str ( xmax ), str ( ymin ), vrt_path , output_path , ] r = subprocess . run ( cmd ) r . check_returncode () These new files can be used in Open Topo Data with the following config.yaml file datasets : - name : eudem25m path : data/eudem-buffered/ filename_epsg : 3035 filename_tile_size : 1000000 after rebuilding: make build && make run","title":"Adding a Buffer to Tiles"},{"location":"notes/cloud-storage/","text":"Storing Datasets in the Cloud You may want to store your elevation datasets on a cloud storage provider (like AWS S3, Google Cloud Storage, or Azure Storage) instead of on a persistent local disk. This configuration can be cheaper than provisioning a server with a lot of disk space, and plays nicely with containerised workflows where services are expected to spin up quickly on standard machine types. Accessing datasets over http will add latency compared reading from a local disk. Open Topo Data doesn't currently have specific support for cloud storage, but there are a few different ways to set this up. Regardless of the approach you take, you probably want to convert your dataset to cloud optimised geotiffs for best performance. Mounting on the host Tools like gcsfuse , s3fuse , and rclone let you mount cloud storage buckets, folders, and files in your local filesystem. You can then point Open Topo Data at the mounted path. For example, you could mount a GCS bucket like mkdir /mnt/otd-cloud-datasets/ gcsfuse www-opentopodata-org-public /mnt/otd-cloud-datasets/ set up the dataset in config.yaml : datasets : - name : srtm-gcloud-subset path : data/test-srtm90m-subset/ then when running the docker image, include the mounted bucket as a volume: docker run --rm -it --volume /mnt/otd-cloud-datasets/:/app/data:ro -p 5000 :5000 opentopodata This is the simplest set and forget way to point Open Topo Data at a dataset living in the cloud. But it requires a long-running host, and access to the host. Mounting inside docker You could do the mounting above inside the docker container, for example by building on the Open Topo Data docker image to add e.g. rclone as a dependency and do the actual mounting. This lets you mount your cloud dataset without modifying the host. However, getting fuse to work inside a docker container can be tricky , and you may not have permissions to do this on some platforms (though it seems possible on GCE ). Building a VRT VRT files are a container format for geospatial rasters, and they support cloud storage through special file paths: for example the path /vsigs/www-opentopodata-org-public/test-srtm90m-subset/N00E010.hgt references the file /test-srtm90m-subset/N00E010.hgt in the www-opentopodata-org-public bucket on Google Cloud Storage. There's a complete list of the special paths in the GDAL docs . Because Open Topo Data understands VRT files, we can build a VRT file wrapping all of the cloud files: gdalbuildvrt data/gcloud/dataset.vrt /vsigs/www-opentopodata-org-public/test-srtm90m-subset/N00E010.hgt /vsigs/www-opentopodata-org-public/test-srtm90m-subset/N00E011.hgt.zip and load this in Open Topo Data as a single file dataset: datasets : - name : srtm-gcloud-subset path : data/gcloud/ finally, you'll need to pass credentials to the docker container: docker run -it -v /home/XXX/opentopodata/data:/app/data:ro -p 5000:5000 -e GS_SECRET_ACCESS_KEY=XXX -e GS_ACCESS_KEY_ID=XXX opentopodata Again the GDAL docs have the format for the credential environment variables.","title":"Cloud storage"},{"location":"notes/cloud-storage/#storing-datasets-in-the-cloud","text":"You may want to store your elevation datasets on a cloud storage provider (like AWS S3, Google Cloud Storage, or Azure Storage) instead of on a persistent local disk. This configuration can be cheaper than provisioning a server with a lot of disk space, and plays nicely with containerised workflows where services are expected to spin up quickly on standard machine types. Accessing datasets over http will add latency compared reading from a local disk. Open Topo Data doesn't currently have specific support for cloud storage, but there are a few different ways to set this up. Regardless of the approach you take, you probably want to convert your dataset to cloud optimised geotiffs for best performance.","title":"Storing Datasets in the Cloud"},{"location":"notes/cloud-storage/#mounting-on-the-host","text":"Tools like gcsfuse , s3fuse , and rclone let you mount cloud storage buckets, folders, and files in your local filesystem. You can then point Open Topo Data at the mounted path. For example, you could mount a GCS bucket like mkdir /mnt/otd-cloud-datasets/ gcsfuse www-opentopodata-org-public /mnt/otd-cloud-datasets/ set up the dataset in config.yaml : datasets : - name : srtm-gcloud-subset path : data/test-srtm90m-subset/ then when running the docker image, include the mounted bucket as a volume: docker run --rm -it --volume /mnt/otd-cloud-datasets/:/app/data:ro -p 5000 :5000 opentopodata This is the simplest set and forget way to point Open Topo Data at a dataset living in the cloud. But it requires a long-running host, and access to the host.","title":"Mounting on the host"},{"location":"notes/cloud-storage/#mounting-inside-docker","text":"You could do the mounting above inside the docker container, for example by building on the Open Topo Data docker image to add e.g. rclone as a dependency and do the actual mounting. This lets you mount your cloud dataset without modifying the host. However, getting fuse to work inside a docker container can be tricky , and you may not have permissions to do this on some platforms (though it seems possible on GCE ).","title":"Mounting inside docker"},{"location":"notes/cloud-storage/#building-a-vrt","text":"VRT files are a container format for geospatial rasters, and they support cloud storage through special file paths: for example the path /vsigs/www-opentopodata-org-public/test-srtm90m-subset/N00E010.hgt references the file /test-srtm90m-subset/N00E010.hgt in the www-opentopodata-org-public bucket on Google Cloud Storage. There's a complete list of the special paths in the GDAL docs . Because Open Topo Data understands VRT files, we can build a VRT file wrapping all of the cloud files: gdalbuildvrt data/gcloud/dataset.vrt /vsigs/www-opentopodata-org-public/test-srtm90m-subset/N00E010.hgt /vsigs/www-opentopodata-org-public/test-srtm90m-subset/N00E011.hgt.zip and load this in Open Topo Data as a single file dataset: datasets : - name : srtm-gcloud-subset path : data/gcloud/ finally, you'll need to pass credentials to the docker container: docker run -it -v /home/XXX/opentopodata/data:/app/data:ro -p 5000:5000 -e GS_SECRET_ACCESS_KEY=XXX -e GS_ACCESS_KEY_ID=XXX opentopodata Again the GDAL docs have the format for the credential environment variables.","title":"Building a VRT"},{"location":"notes/dataset-sizes/","text":"Dataset sizes The table below lists the file sizes of the datasets on the public APIs server. In most cases I converted the source files to compressed geotiffs, and the sizes given are after that conversion. A freshly downloaded dataset could be much larger depending on the format, but compressing with gdal should result in a size similar to what I ended up with. API id Dataset Compressed File Size Notes bkg200 BKG 200m, Germany 32 MB etopo1 ETOPO 1 arcminute land and bathymetry 1 GB gebco2020 GEBCO global bathymetry (2020) 3 GB emod2018 EMOD Europe bathymetry (2018) 3 GB After conversion from .asc to compressed .geotiff . nzdem8m New Zealand 8m DEM 9 GB srtm90m SRTM ~90m 12 GB After conversion from .hgt to compressed .geotiff . eudem25m Europe 25m DEM 22 GB srtm30m SRTM ~30m 73 GB mapzen Mapzen ~30m 142 GB After conversion from .hgt format to compressed .geotiff . The source files are 100s of GB larger. aster30m ASTER ~30m 151 GB ned10m US National Elevation Dataset (1/3 arcsecond) 260 GB","title":"Dataset sizes"},{"location":"notes/dataset-sizes/#dataset-sizes","text":"The table below lists the file sizes of the datasets on the public APIs server. In most cases I converted the source files to compressed geotiffs, and the sizes given are after that conversion. A freshly downloaded dataset could be much larger depending on the format, but compressing with gdal should result in a size similar to what I ended up with. API id Dataset Compressed File Size Notes bkg200 BKG 200m, Germany 32 MB etopo1 ETOPO 1 arcminute land and bathymetry 1 GB gebco2020 GEBCO global bathymetry (2020) 3 GB emod2018 EMOD Europe bathymetry (2018) 3 GB After conversion from .asc to compressed .geotiff . nzdem8m New Zealand 8m DEM 9 GB srtm90m SRTM ~90m 12 GB After conversion from .hgt to compressed .geotiff . eudem25m Europe 25m DEM 22 GB srtm30m SRTM ~30m 73 GB mapzen Mapzen ~30m 142 GB After conversion from .hgt format to compressed .geotiff . The source files are 100s of GB larger. aster30m ASTER ~30m 151 GB ned10m US National Elevation Dataset (1/3 arcsecond) 260 GB","title":"Dataset sizes"},{"location":"notes/invalid-srtm-zips/","text":"Invalid SRTM zips over the Caspian sea If you're working with SRTM tiles in .hgt.zip format you might have found some files that can't be processed by gdal (and therefore can't be processed by Open Topo Data): gdalinfo N44E049.SRTMGL1.hgt.zip ERROR 4: `N44E049.SRTMGL1.hgt.zip' not recognized as a supported file format. gdalinfo failed - unable to open 'N44E049.SRTMGL1.hgt.zip'. There are 16 tiles with this issue N37E051 N37E052 N38E050 N38E051 N38E052 N39E050 N39E051 N40E051 N41E050 N41E051 N42E049 N42E050 N43E048 N43E049 N44E048 N44E049 all of which cover no-land areas of the Caspian Sea Invalid tiles shown in pink. Basemap is Bing aerial imagery. and have a constant elevation of -29m gdalinfo -mm N44E049.hgt | grep Min Computed Min/Max=-29.000,-29.000 What's the issue Normal .hgt.zip files are a zip archive containing a file named like NxxEyyy.hgt unzip -l N37E011.SRTMGL1.hgt.zip Archive: N37E011.SRTMGL1.hgt.zip Length Date Time Name --------- ---------- ----- ---- 25934402 2012-10-08 15:44 N37E011.hgt --------- ------- 25934402 1 file But the zips over the Caspian sea contain a file named like NxxEyyy.SRTMGL1.hgt unzip -l N44E049.SRTMGL1.hgt.zip Archive: N44E049.SRTMGL1.hgt.zip Length Date Time Name --------- ---------- ----- ---- 25934402 2015-08-10 10:24 N44E049.SRTMGL1.hgt --------- ------- 25934402 1 file and unfortunately while GDAL will read NxxEyyy[.something].hgt and NxxEyyy[.something].hgt.zip files just fine, the .hgt file inside the .zip file must be named NxxEyyy.hgt . Because these tiles all have a constant value (the elevation of the Caspian sea, which is about -29m) and weren't even included in the previous version of SRTM, I'm guessing the files went through a different pipeline to the rest of the datasets. I'm not sure if this is a bug with gdal or the dataset itself. How to fix these files The 16 invalid tiles can be fixed by renaming the .hgt file contained within: unzip N44E049.SRTMGL1.hgt.zip mv N44E049.SRTMGL1.hgt N44E049.hgt zip N44E049.SRTMGL1.repacked.hgt.zip N44E049.hgt Which datasets are affected Both the 30m and 90m resolutions of SRTM version 3 hosted on usgs.gov/MEASURES are affected. Version 2 of SRTM just excludes the 16 tiles over the Caspian sea: dds.cr.usgs.gov/srtm/version2_1/SRTM3/Eurasia/ The Cigar version 4.1 of SRTM has the correct elevation.","title":"Invalid SRTM zips"},{"location":"notes/invalid-srtm-zips/#invalid-srtm-zips-over-the-caspian-sea","text":"If you're working with SRTM tiles in .hgt.zip format you might have found some files that can't be processed by gdal (and therefore can't be processed by Open Topo Data): gdalinfo N44E049.SRTMGL1.hgt.zip ERROR 4: `N44E049.SRTMGL1.hgt.zip' not recognized as a supported file format. gdalinfo failed - unable to open 'N44E049.SRTMGL1.hgt.zip'. There are 16 tiles with this issue N37E051 N37E052 N38E050 N38E051 N38E052 N39E050 N39E051 N40E051 N41E050 N41E051 N42E049 N42E050 N43E048 N43E049 N44E048 N44E049 all of which cover no-land areas of the Caspian Sea Invalid tiles shown in pink. Basemap is Bing aerial imagery. and have a constant elevation of -29m gdalinfo -mm N44E049.hgt | grep Min Computed Min/Max=-29.000,-29.000","title":"Invalid SRTM zips over the Caspian sea"},{"location":"notes/invalid-srtm-zips/#whats-the-issue","text":"Normal .hgt.zip files are a zip archive containing a file named like NxxEyyy.hgt unzip -l N37E011.SRTMGL1.hgt.zip Archive: N37E011.SRTMGL1.hgt.zip Length Date Time Name --------- ---------- ----- ---- 25934402 2012-10-08 15:44 N37E011.hgt --------- ------- 25934402 1 file But the zips over the Caspian sea contain a file named like NxxEyyy.SRTMGL1.hgt unzip -l N44E049.SRTMGL1.hgt.zip Archive: N44E049.SRTMGL1.hgt.zip Length Date Time Name --------- ---------- ----- ---- 25934402 2015-08-10 10:24 N44E049.SRTMGL1.hgt --------- ------- 25934402 1 file and unfortunately while GDAL will read NxxEyyy[.something].hgt and NxxEyyy[.something].hgt.zip files just fine, the .hgt file inside the .zip file must be named NxxEyyy.hgt . Because these tiles all have a constant value (the elevation of the Caspian sea, which is about -29m) and weren't even included in the previous version of SRTM, I'm guessing the files went through a different pipeline to the rest of the datasets. I'm not sure if this is a bug with gdal or the dataset itself.","title":"What's the issue"},{"location":"notes/invalid-srtm-zips/#how-to-fix-these-files","text":"The 16 invalid tiles can be fixed by renaming the .hgt file contained within: unzip N44E049.SRTMGL1.hgt.zip mv N44E049.SRTMGL1.hgt N44E049.hgt zip N44E049.SRTMGL1.repacked.hgt.zip N44E049.hgt","title":"How to fix these files"},{"location":"notes/invalid-srtm-zips/#which-datasets-are-affected","text":"Both the 30m and 90m resolutions of SRTM version 3 hosted on usgs.gov/MEASURES are affected. Version 2 of SRTM just excludes the 16 tiles over the Caspian sea: dds.cr.usgs.gov/srtm/version2_1/SRTM3/Eurasia/ The Cigar version 4.1 of SRTM has the correct elevation.","title":"Which datasets are affected"},{"location":"notes/kubernetes/","text":"How to deploy on Kubernetes This example shows how to deploy to Kubernetes using the base opentopodata docker image. It includes a workload (ingress.yml) configuration and a service (service.yml) configuration which is used to access the opentopodata API by routing queries to the container. Prerequisites You need access to a K8s cluster and in this example we are using the command-line-interface kubectl to deploy. For instructions on how to do that visit the K8s website . Example Assuming you have a domain, subdomain.example.com where you want to make the opentopodata available on the endpoint subdomain.example.com/dem-api/ , the files deployment.yml , ingress.yml and service.yml shows how to set that up. It works out of the box with the base opentopodata docker image. Simply run these commands: kubectl --namespace my-namespace apply -f service.yml kubectl --namespace my-namespace apply -f ingress.yml kubectl --namespace my-namespace apply -f deployment.yml service.yml apiVersion : v1 kind : Service metadata : name : dem-api labels : service : dem-api spec : selector : deploy : dem-api ports : - port : 5000 targetPort : 5000 ingress.yml apiVersion : extensions/v1beta1 kind : Ingress metadata : name : dem-api annotations : nginx.ingress.kubernetes.io/rewrite-target : /$1 nginx.ingress.kubernetes.io/ssl-redirect : \"false\" spec : rules : - host : subdomain.example.com http : paths : - path : /dem-api/(.*) backend : serviceName : dem-api servicePort : 5000 deployment.yml apiVersion : apps/v1 kind : Deployment metadata : name : dem-api spec : replicas : 1 selector : matchLabels : deploy : dem-api template : metadata : labels : deploy : dem-api spec : containers : - image : opentopodata name : dem-api imagePullPolicy : Always ports : - containerPort : 5000 restartPolicy : Always Thanks to @khintz for contributing this documentation in #57 !","title":"Kubernetes"},{"location":"notes/kubernetes/#how-to-deploy-on-kubernetes","text":"This example shows how to deploy to Kubernetes using the base opentopodata docker image. It includes a workload (ingress.yml) configuration and a service (service.yml) configuration which is used to access the opentopodata API by routing queries to the container.","title":"How to deploy on Kubernetes"},{"location":"notes/kubernetes/#prerequisites","text":"You need access to a K8s cluster and in this example we are using the command-line-interface kubectl to deploy. For instructions on how to do that visit the K8s website .","title":"Prerequisites"},{"location":"notes/kubernetes/#example","text":"Assuming you have a domain, subdomain.example.com where you want to make the opentopodata available on the endpoint subdomain.example.com/dem-api/ , the files deployment.yml , ingress.yml and service.yml shows how to set that up. It works out of the box with the base opentopodata docker image. Simply run these commands: kubectl --namespace my-namespace apply -f service.yml kubectl --namespace my-namespace apply -f ingress.yml kubectl --namespace my-namespace apply -f deployment.yml","title":"Example"},{"location":"notes/kubernetes/#serviceyml","text":"apiVersion : v1 kind : Service metadata : name : dem-api labels : service : dem-api spec : selector : deploy : dem-api ports : - port : 5000 targetPort : 5000","title":"service.yml"},{"location":"notes/kubernetes/#ingressyml","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : dem-api annotations : nginx.ingress.kubernetes.io/rewrite-target : /$1 nginx.ingress.kubernetes.io/ssl-redirect : \"false\" spec : rules : - host : subdomain.example.com http : paths : - path : /dem-api/(.*) backend : serviceName : dem-api servicePort : 5000","title":"ingress.yml"},{"location":"notes/kubernetes/#deploymentyml","text":"apiVersion : apps/v1 kind : Deployment metadata : name : dem-api spec : replicas : 1 selector : matchLabels : deploy : dem-api template : metadata : labels : deploy : dem-api spec : containers : - image : opentopodata name : dem-api imagePullPolicy : Always ports : - containerPort : 5000 restartPolicy : Always Thanks to @khintz for contributing this documentation in #57 !","title":"deployment.yml"},{"location":"notes/multiple-datasets/","text":"Querying multiple datasets From v1.5.0, Open Topo Data has the ability to request the \"best\" elevation from multiple datasets in a single query. I'm still improving the functionality, please open an issue if you find a problem. Warning While Open Topo Data lets you query multiple datasets seamlessly, it doesn't ensure that the datasets you're using share the same vertical datum or have seamless transitions. A great usecase for Multi Datasets would be if your usage is mostly focussed on an area with contiguous DEM coverage (like the island nation of New Zealand), but you want to technically be able to query worldwide. A bad usecase for Multi Datasets is layering a bunch of patchy 10cm lidar data on top of a 1.8km base DEM and using it to draw small scale elevation profiles. Say your config.yaml looks like this: # Hi-res New Zealand. - name : nzdem8m path : data/nzdem8m/ filename_tile_size : 65536 filename_epsg : 2193 # Mapzen global. - name : mapzen path : data/mapzen/ You want the following: If you query a location in New Zealand, return the hi-res nzdem result. If you query a location in New Zealand and nzdem is null at that location, return the mapzen result for that location. If you query a location outside New Zealand, return the mapzen result for that location. There are two ways to do this. Putting multiple datasets in the url curl https://api.opentopodata.org/v1/nzdem8m,mapzen?locations = -43.801,172.968 | -18.143,178.444 { \"results\" : [ { \"dataset\" : \"nzdem8m\" , \"elevation\" : 1.4081547260284424 , \"location\" : { \"lat\" : -43.801 , \"lng\" : 172.968 } }, { \"dataset\" : \"mapzen\" , \"elevation\" : 23.0 , \"location\" : { \"lat\" : -18.143 , \"lng\" : 178.444 } } ], \"status\" : \"OK\" } For each location, the first non-null dataset elevation is returned. Defining multi dataset in config.yaml If you have a lot of datasets you want to merge, it's a pain to put them all in the url. Instead you can define a MultiDataset in config.yaml : # Hi-res New Zealand. - name : nzdem8m path : data/nzdem8m/ filename_tile_size : 65536 filename_epsg : 2193 # Mapzen global. - name : mapzen path : data/mapzen/ # NZ with mapzen fallback. - name : nz-global child_datasets : - nzdem8m - mapzen Now you can query nz-global for the same result. curl https://api.opentopodata.org/v1/nz-global?locations = -43.801,172.968 | -18.143,178.444 { \"results\" : [ { \"dataset\" : \"nzdem8m\" , \"elevation\" : 1.4081547260284424 , \"location\" : { \"lat\" : -43.801 , \"lng\" : 172.968 } }, { \"dataset\" : \"mapzen\" , \"elevation\" : 23.0 , \"location\" : { \"lat\" : -18.143 , \"lng\" : 178.444 } } ], \"status\" : \"OK\" } Performance optimisation Querying multiple datasets is more performance-intensive than querying a single one. For a Multi Dataset with 10 child datasets where the first 9 are null at the queried location, Open Topo Data will have to cover the lat/lon coordinates into the CRS of all 10 datasets to try to find a tile. Worst case, if those 10 tiles exist, Open Topo Data will have to open and read from all 10 files sequentially. To reduce the number of checks Open Topo Data has to do, you can manually specify the bounds of each dataset, in WGS84 format. If a location is outside those bounds, Open Topo Data doesn't need to check that dataset. If your config.yaml file looks like # Hi-res New Zealand. - name : nzdem8m path : data/nzdem8m/ filename_tile_size : 65536 filename_epsg : 2193 wgs84_bounds : left : 165 right : 180 bottom : -48 top : -33 # Mapzen global. - name : mapzen path : data/mapzen/ then querying nzdem8m,mapzen with a location in Europe will go straight to mapzen without needing to convert the location to epsg:2193 and check for a tile.","title":"Multiple datasets"},{"location":"notes/multiple-datasets/#querying-multiple-datasets","text":"From v1.5.0, Open Topo Data has the ability to request the \"best\" elevation from multiple datasets in a single query. I'm still improving the functionality, please open an issue if you find a problem. Warning While Open Topo Data lets you query multiple datasets seamlessly, it doesn't ensure that the datasets you're using share the same vertical datum or have seamless transitions. A great usecase for Multi Datasets would be if your usage is mostly focussed on an area with contiguous DEM coverage (like the island nation of New Zealand), but you want to technically be able to query worldwide. A bad usecase for Multi Datasets is layering a bunch of patchy 10cm lidar data on top of a 1.8km base DEM and using it to draw small scale elevation profiles. Say your config.yaml looks like this: # Hi-res New Zealand. - name : nzdem8m path : data/nzdem8m/ filename_tile_size : 65536 filename_epsg : 2193 # Mapzen global. - name : mapzen path : data/mapzen/ You want the following: If you query a location in New Zealand, return the hi-res nzdem result. If you query a location in New Zealand and nzdem is null at that location, return the mapzen result for that location. If you query a location outside New Zealand, return the mapzen result for that location. There are two ways to do this.","title":"Querying multiple datasets"},{"location":"notes/multiple-datasets/#putting-multiple-datasets-in-the-url","text":"curl https://api.opentopodata.org/v1/nzdem8m,mapzen?locations = -43.801,172.968 | -18.143,178.444 { \"results\" : [ { \"dataset\" : \"nzdem8m\" , \"elevation\" : 1.4081547260284424 , \"location\" : { \"lat\" : -43.801 , \"lng\" : 172.968 } }, { \"dataset\" : \"mapzen\" , \"elevation\" : 23.0 , \"location\" : { \"lat\" : -18.143 , \"lng\" : 178.444 } } ], \"status\" : \"OK\" } For each location, the first non-null dataset elevation is returned.","title":"Putting multiple datasets in the url"},{"location":"notes/multiple-datasets/#defining-multi-dataset-in-configyaml","text":"If you have a lot of datasets you want to merge, it's a pain to put them all in the url. Instead you can define a MultiDataset in config.yaml : # Hi-res New Zealand. - name : nzdem8m path : data/nzdem8m/ filename_tile_size : 65536 filename_epsg : 2193 # Mapzen global. - name : mapzen path : data/mapzen/ # NZ with mapzen fallback. - name : nz-global child_datasets : - nzdem8m - mapzen Now you can query nz-global for the same result. curl https://api.opentopodata.org/v1/nz-global?locations = -43.801,172.968 | -18.143,178.444 { \"results\" : [ { \"dataset\" : \"nzdem8m\" , \"elevation\" : 1.4081547260284424 , \"location\" : { \"lat\" : -43.801 , \"lng\" : 172.968 } }, { \"dataset\" : \"mapzen\" , \"elevation\" : 23.0 , \"location\" : { \"lat\" : -18.143 , \"lng\" : 178.444 } } ], \"status\" : \"OK\" }","title":"Defining multi dataset in config.yaml"},{"location":"notes/multiple-datasets/#performance-optimisation","text":"Querying multiple datasets is more performance-intensive than querying a single one. For a Multi Dataset with 10 child datasets where the first 9 are null at the queried location, Open Topo Data will have to cover the lat/lon coordinates into the CRS of all 10 datasets to try to find a tile. Worst case, if those 10 tiles exist, Open Topo Data will have to open and read from all 10 files sequentially. To reduce the number of checks Open Topo Data has to do, you can manually specify the bounds of each dataset, in WGS84 format. If a location is outside those bounds, Open Topo Data doesn't need to check that dataset. If your config.yaml file looks like # Hi-res New Zealand. - name : nzdem8m path : data/nzdem8m/ filename_tile_size : 65536 filename_epsg : 2193 wgs84_bounds : left : 165 right : 180 bottom : -48 top : -33 # Mapzen global. - name : mapzen path : data/mapzen/ then querying nzdem8m,mapzen with a location in Europe will go straight to mapzen without needing to convert the location to epsg:2193 and check for a tile.","title":"Performance optimisation"},{"location":"notes/performance-optimisation/","text":"Performance optimisation If you're self-hosting Open Topo Data and want better throughput or to run on a cheaper instance there's a few things you can do. Server hardware Open Topo Data is mostly CPU bound. It benefits from fast CPUs and high virtual core count. A spinning hard drive is enough to saturate the CPU so an SSD is not needed. Using a network filestore or keeping tiles in cloud storage will probably introduce enough latency to become the bottleneck. It uses very little memory. Queries Batch request are faster (per point queried) than single-point requests, and large batches are faster than small ones. Increase max_locations_per_request to as much as you can fit in a a URL. Batch queries are fastest if the points are located next to each other. Sorting the locations you are querying before batching will improve performance. Ideally sort by some block-level attribute like postal code or state/county/region, or by something like round(lat, 1), round(lon, 1) depending on your tile size. If the requests are very large and the server has several CPU cores, try splitting the request and sending it simultaneously. The optimum for the number of requests is slightly higher than the amount of CPU cores used by Open Topo Data. The number of CPU cores used is displayed when OpenTopodata is started. If you missed the log message, you can find iw with the following command: docker logs { NAME_OF_CONTAINER } 2 > & 1 | grep \"CPU cores\" Dataset format A request spends 90% of its time reading the dataset, so the format your raster tiles are in can greatly impact performance. The most optimal way to store rasters depends heavily on the dataset, but here's some rules of thumb: Tile size Files around 2,000 to 20,000 are pixels square are good. Too small and you end up opening lots of files for batch requests; too large and read time is slower. Format Actually this one is clear: use GeoTIFFs. I haven't found anything faster (supported by GDAL) for any of the datasets in the public API. Compression GDAL options -co PREDICTOR=1 is often fastest to read, though can make files larger. -co COMPRESS=DEFLATE is often fastest to read, though can be larger than lzw and zstd especially for floating point data. -co ZLEVEL=1 gives a small read performance boost, makes writing noticeably faster, while barely increasing size. All of the above are minor differences compared to using uncompressed GeoTIFFs or other formats, don't stress it. Open Topo Data doesn't support zstd (as it's not supported yet by rasterio and compiling GDAL from source greatly increases build times) but there's an old branch zstd that has support Multiple datasets Add tight wgs84_bounds for multiple datasets. If your dataset isn't a filled rectangle (e.g., you have one dataset covering CONUS, AK, and HI but not Canada) you might want to split it into multiple datasets with tight bounds. Version Open Topo Data gets faster each release, either though performance improvements or from updated dependencies. Use the latest version. This is especially true if you're using a version older than 1.5.0, as this release gives a 2x+ speedup. Testing performance You can easily test throughput using ab : ab -n 500 -c 8 http://localhost:5000/v1/test-dataset?locations = 56 ,123 You should test on your particular dataset and batch size. It doesn't seem to matter much if you use a fixed url or build a list with different urls for each request: there's no response caching (though your OS may cache files and GDAL may cache raster blocks.) Benchmark results Here are some plots I made benchmarking version 1.5.0 with 8m NZ DEM. The specific results probably won't apply to your dataset, and I included the general takeaways above. Response time grows sublinearly with batch size. Querying locations that lie on the same tile is 2x faster than locations over multiple tiles. Read time and file size for different GeoTIFF compression methods.","title":"Performance optimisation"},{"location":"notes/performance-optimisation/#performance-optimisation","text":"If you're self-hosting Open Topo Data and want better throughput or to run on a cheaper instance there's a few things you can do.","title":"Performance optimisation"},{"location":"notes/performance-optimisation/#server-hardware","text":"Open Topo Data is mostly CPU bound. It benefits from fast CPUs and high virtual core count. A spinning hard drive is enough to saturate the CPU so an SSD is not needed. Using a network filestore or keeping tiles in cloud storage will probably introduce enough latency to become the bottleneck. It uses very little memory.","title":"Server hardware"},{"location":"notes/performance-optimisation/#queries","text":"Batch request are faster (per point queried) than single-point requests, and large batches are faster than small ones. Increase max_locations_per_request to as much as you can fit in a a URL. Batch queries are fastest if the points are located next to each other. Sorting the locations you are querying before batching will improve performance. Ideally sort by some block-level attribute like postal code or state/county/region, or by something like round(lat, 1), round(lon, 1) depending on your tile size. If the requests are very large and the server has several CPU cores, try splitting the request and sending it simultaneously. The optimum for the number of requests is slightly higher than the amount of CPU cores used by Open Topo Data. The number of CPU cores used is displayed when OpenTopodata is started. If you missed the log message, you can find iw with the following command: docker logs { NAME_OF_CONTAINER } 2 > & 1 | grep \"CPU cores\"","title":"Queries"},{"location":"notes/performance-optimisation/#dataset-format","text":"A request spends 90% of its time reading the dataset, so the format your raster tiles are in can greatly impact performance. The most optimal way to store rasters depends heavily on the dataset, but here's some rules of thumb: Tile size Files around 2,000 to 20,000 are pixels square are good. Too small and you end up opening lots of files for batch requests; too large and read time is slower. Format Actually this one is clear: use GeoTIFFs. I haven't found anything faster (supported by GDAL) for any of the datasets in the public API. Compression GDAL options -co PREDICTOR=1 is often fastest to read, though can make files larger. -co COMPRESS=DEFLATE is often fastest to read, though can be larger than lzw and zstd especially for floating point data. -co ZLEVEL=1 gives a small read performance boost, makes writing noticeably faster, while barely increasing size. All of the above are minor differences compared to using uncompressed GeoTIFFs or other formats, don't stress it. Open Topo Data doesn't support zstd (as it's not supported yet by rasterio and compiling GDAL from source greatly increases build times) but there's an old branch zstd that has support","title":"Dataset format"},{"location":"notes/performance-optimisation/#multiple-datasets","text":"Add tight wgs84_bounds for multiple datasets. If your dataset isn't a filled rectangle (e.g., you have one dataset covering CONUS, AK, and HI but not Canada) you might want to split it into multiple datasets with tight bounds.","title":"Multiple datasets"},{"location":"notes/performance-optimisation/#version","text":"Open Topo Data gets faster each release, either though performance improvements or from updated dependencies. Use the latest version. This is especially true if you're using a version older than 1.5.0, as this release gives a 2x+ speedup.","title":"Version"},{"location":"notes/performance-optimisation/#testing-performance","text":"You can easily test throughput using ab : ab -n 500 -c 8 http://localhost:5000/v1/test-dataset?locations = 56 ,123 You should test on your particular dataset and batch size. It doesn't seem to matter much if you use a fixed url or build a list with different urls for each request: there's no response caching (though your OS may cache files and GDAL may cache raster blocks.)","title":"Testing performance"},{"location":"notes/performance-optimisation/#benchmark-results","text":"Here are some plots I made benchmarking version 1.5.0 with 8m NZ DEM. The specific results probably won't apply to your dataset, and I included the general takeaways above. Response time grows sublinearly with batch size. Querying locations that lie on the same tile is 2x faster than locations over multiple tiles. Read time and file size for different GeoTIFF compression methods.","title":"Benchmark results"},{"location":"notes/running-without-docker/","text":"Running without Docker Open Topo Data uses docker to manage dependencies, builds, and processes. Containerisation is especially helpful in the geospatial domain, where compatibility between system libraries, compiled packages, and python scripts is flakey. So I highly recommend running Open Topo Data with docker, it saves me a bunch of headaches. But if you've read this far you already know that \ud83d\udc09. Running Open Topo Data 1.5.0 on Debian 10 A user (thanks Luca !) was able to get Open Topo Data running on Debian 10 without docker and was kind enough to share their instructions. Minimal install Download Open Topo Data. git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata Install system dependencies (if you're not using Debian 10, install whatever python3.X-dev matches your installed python)L apt install gcc python3.7-dev python3-pip Debian 10 comes with an old version of pip, it needs to be updated so we can install wheels: pip3 install --upgrade pip For some reason pyproj needs to be installed on its own, otherwise it will use the outdated system PROJ library instead of the packaged wheel version. Find the version of pyproj required cat requirements.txt | grep pyproj and install that pinned version pip3 install pyproj == 3 .4.1 then the remaining python packages can be installed: pip3 install -r requirements.txt This should give a minimal install of Open Topo Data that can be started with FLASK_APP = opentopodata/api.py DISABLE_MEMCACHE = 1 flask run --port 5000 Full install The minimal instructions above install Open Topo Data without memcache or a web server. This is fine if you have a small dataset, few requests per second, and don't expose the insecure flask server to the internet. For a faster and more secure server, you can install memcache and uwsgi, and run the service with systemd. Install some more dependencies: apt install memcached pip3 install regex uwsgi Set up memcached. On Debian 10, memcached comes with \"PrivateTemp\" enabled, which prevents saving the socket where Open Topo Data expects it: usermod -g www-data memcache mkdir -p /etc/systemd/system/memcached.service.d/ echo -e \"[Service]\\n\\nPrivateTmp=false\" > /etc/systemd/system/memcached.service.d/override.conf systemctl daemon-reload echo -e \"-s /tmp/memcached.sock\\n-a 0775\\n-c 1024\\n-I 8m\" >> /etc/memcached.conf service memcached restart Create a file uwsgi.ini somewhere, say /home/opentopodata/uwsgi.ini , that points to the repo you downloaded: [uwsgi] strict = true need-app = true http-socket = :9090 vacuum = true uid = www-data gid = www-data master = true chdir = /home/opentopodata pythonpath = /home/opentopodata wsgi-file = /home/opentopodata/opentopodata/api.py callable = app manage-script-name = true die-on-term = true buffer-size = 65535 If uwsgi works with /usr/local/bin/uwsgi --ini /home/opentopodata/uwsgi.ini --processes 10s Then you can create a systemd script in /etc/systemd/system/opentopodata.service : [Unit] Description = OpenTopoData web application After = network.target [Service] User = www-data WorkingDirectory = /home/opentopodata ExecStart = /usr/local/bin/uwsgi /home/opentopodata/uwsgi.ini --processes 10s Restart = always [Install] WantedBy = multi-user.target Then manage Open Topo Data with systemctl daemon-reload systemctl enable opentopodata.service systemctl start opentopodata.service Warning Opentopodata caches config.yaml in two places: memcache and uwsgi. If you update the config file (to eg add a new dataset) you'll need to restart memcached first , then opentopodata.","title":"Run without docker"},{"location":"notes/running-without-docker/#running-without-docker","text":"Open Topo Data uses docker to manage dependencies, builds, and processes. Containerisation is especially helpful in the geospatial domain, where compatibility between system libraries, compiled packages, and python scripts is flakey. So I highly recommend running Open Topo Data with docker, it saves me a bunch of headaches. But if you've read this far you already know that \ud83d\udc09.","title":"Running without Docker"},{"location":"notes/running-without-docker/#running-open-topo-data-150-on-debian-10","text":"A user (thanks Luca !) was able to get Open Topo Data running on Debian 10 without docker and was kind enough to share their instructions.","title":"Running Open Topo Data 1.5.0 on Debian 10"},{"location":"notes/running-without-docker/#minimal-install","text":"Download Open Topo Data. git clone https://github.com/ajnisbet/opentopodata.git cd opentopodata Install system dependencies (if you're not using Debian 10, install whatever python3.X-dev matches your installed python)L apt install gcc python3.7-dev python3-pip Debian 10 comes with an old version of pip, it needs to be updated so we can install wheels: pip3 install --upgrade pip For some reason pyproj needs to be installed on its own, otherwise it will use the outdated system PROJ library instead of the packaged wheel version. Find the version of pyproj required cat requirements.txt | grep pyproj and install that pinned version pip3 install pyproj == 3 .4.1 then the remaining python packages can be installed: pip3 install -r requirements.txt This should give a minimal install of Open Topo Data that can be started with FLASK_APP = opentopodata/api.py DISABLE_MEMCACHE = 1 flask run --port 5000","title":"Minimal install"},{"location":"notes/running-without-docker/#full-install","text":"The minimal instructions above install Open Topo Data without memcache or a web server. This is fine if you have a small dataset, few requests per second, and don't expose the insecure flask server to the internet. For a faster and more secure server, you can install memcache and uwsgi, and run the service with systemd. Install some more dependencies: apt install memcached pip3 install regex uwsgi Set up memcached. On Debian 10, memcached comes with \"PrivateTemp\" enabled, which prevents saving the socket where Open Topo Data expects it: usermod -g www-data memcache mkdir -p /etc/systemd/system/memcached.service.d/ echo -e \"[Service]\\n\\nPrivateTmp=false\" > /etc/systemd/system/memcached.service.d/override.conf systemctl daemon-reload echo -e \"-s /tmp/memcached.sock\\n-a 0775\\n-c 1024\\n-I 8m\" >> /etc/memcached.conf service memcached restart Create a file uwsgi.ini somewhere, say /home/opentopodata/uwsgi.ini , that points to the repo you downloaded: [uwsgi] strict = true need-app = true http-socket = :9090 vacuum = true uid = www-data gid = www-data master = true chdir = /home/opentopodata pythonpath = /home/opentopodata wsgi-file = /home/opentopodata/opentopodata/api.py callable = app manage-script-name = true die-on-term = true buffer-size = 65535 If uwsgi works with /usr/local/bin/uwsgi --ini /home/opentopodata/uwsgi.ini --processes 10s Then you can create a systemd script in /etc/systemd/system/opentopodata.service : [Unit] Description = OpenTopoData web application After = network.target [Service] User = www-data WorkingDirectory = /home/opentopodata ExecStart = /usr/local/bin/uwsgi /home/opentopodata/uwsgi.ini --processes 10s Restart = always [Install] WantedBy = multi-user.target Then manage Open Topo Data with systemctl daemon-reload systemctl enable opentopodata.service systemctl start opentopodata.service Warning Opentopodata caches config.yaml in two places: memcache and uwsgi. If you update the config file (to eg add a new dataset) you'll need to restart memcached first , then opentopodata.","title":"Full install"},{"location":"notes/windows-support/","text":"Running Open Topo Data on Windows A few users have mentioned that the getting started instructions don't work on Windows. This is difficult for me to debug without access Windows machine, but I believe the instructions below will work. Getting started on Windows Install docker and git . Make sure docker is running. The command docker ps should print a (possibly empty) table of containers, rather than an error message. Clone the repository with git clone https://github.com/ajnisbet/opentopodata.git and change into the repo folder cd opentopodata To build the docker image, instead of using of make build , build with docker build --tag opentopodata --file docker/Dockerfile . To run the server, instead of using make run , run with docker run --rm -it --volume \"C:/path/to/opentopodata/data:/app/data:ro\" -p 5000:5000 -e N_UWSGI_THREADS=8 opentopodata sh -c \"/usr/bin/supervisord -c /app/docker/supervisord.conf\" Modify -e N_UWSGI_THREADS=8 in the docker run command above with the number of logical CPU cores on your system. Open Topo Data is CPU bound for most compressed datasets. Also modify C:/path/to/opentopodata/data in that command with the path to the folder containing your datasets. Troubleshooting Error during connect An error like error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.40/containers/json: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running. means docker isn't running. The input device is not a TTY Trying to run the docker run command in Git CMD gives me this error: the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty' Run the docker run command in Command Prompt instead. Could not find config file Errors complaining about not finding /app/docker/supervisord.conf like process_begin: CreateProcess(NULL, pwd, ...) failed. Makefile:8: pipe: No such file or directory Error: could not find config file /app/docker/supervisord.conf For help, use /usr/bin/supervisord -h make: *** [Makefile:8: run] Error 2 or docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"exec: \\\"exec /usr/bin/supervisord -c /app/docker/supervisord.conf\\\": stat exec /usr/bin/supervisord -c /app/docker/supervisord.conf: no such file or directory\": unknown. should be fixed in v1.3.1 and greater, or might mean you're using make instead of the commands above.","title":"Windows support"},{"location":"notes/windows-support/#running-open-topo-data-on-windows","text":"A few users have mentioned that the getting started instructions don't work on Windows. This is difficult for me to debug without access Windows machine, but I believe the instructions below will work.","title":"Running Open Topo Data on Windows"},{"location":"notes/windows-support/#getting-started-on-windows","text":"Install docker and git . Make sure docker is running. The command docker ps should print a (possibly empty) table of containers, rather than an error message. Clone the repository with git clone https://github.com/ajnisbet/opentopodata.git and change into the repo folder cd opentopodata To build the docker image, instead of using of make build , build with docker build --tag opentopodata --file docker/Dockerfile . To run the server, instead of using make run , run with docker run --rm -it --volume \"C:/path/to/opentopodata/data:/app/data:ro\" -p 5000:5000 -e N_UWSGI_THREADS=8 opentopodata sh -c \"/usr/bin/supervisord -c /app/docker/supervisord.conf\" Modify -e N_UWSGI_THREADS=8 in the docker run command above with the number of logical CPU cores on your system. Open Topo Data is CPU bound for most compressed datasets. Also modify C:/path/to/opentopodata/data in that command with the path to the folder containing your datasets.","title":"Getting started on Windows"},{"location":"notes/windows-support/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"notes/windows-support/#error-during-connect","text":"An error like error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.40/containers/json: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running. means docker isn't running.","title":"Error during connect"},{"location":"notes/windows-support/#the-input-device-is-not-a-tty","text":"Trying to run the docker run command in Git CMD gives me this error: the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty' Run the docker run command in Command Prompt instead.","title":"The input device is not a TTY"},{"location":"notes/windows-support/#could-not-find-config-file","text":"Errors complaining about not finding /app/docker/supervisord.conf like process_begin: CreateProcess(NULL, pwd, ...) failed. Makefile:8: pipe: No such file or directory Error: could not find config file /app/docker/supervisord.conf For help, use /usr/bin/supervisord -h make: *** [Makefile:8: run] Error 2 or docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"exec: \\\"exec /usr/bin/supervisord -c /app/docker/supervisord.conf\\\": stat exec /usr/bin/supervisord -c /app/docker/supervisord.conf: no such file or directory\": unknown. should be fixed in v1.3.1 and greater, or might mean you're using make instead of the commands above.","title":"Could not find config file"}]}